"""
Agent Mode System - TRUE AUTONOMOUS AI Data Analysis Pipeline
Provides intelligent decision-making, adaptive strategies, and self-optimization.
"""

import asyncio
import threading
import time
import traceback
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Callable
from pathlib import Path
from collections import defaultdict
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

class AgentDecisionEngine:
    """
    Intelligent decision engine that makes autonomous choices based on data characteristics.
    """
    
    def __init__(self):
        self.knowledge_base = self._load_knowledge_base()
        self.decision_history = []
        self.performance_metrics = {}
        
    def _load_knowledge_base(self) -> Dict:
        """Load or initialize knowledge base from previous runs."""
        kb_path = Path("ai_analytics_storage/agent_knowledge_base.json")
        if kb_path.exists():
            try:
                with open(kb_path, 'r') as f:
                    return json.load(f)
            except:
                pass
        
        # Default knowledge base
        return {
            'successful_strategies': defaultdict(list),
            'failed_strategies': defaultdict(list),
            'dataset_profiles': [],
            'optimal_algorithms': {},
            'processing_times': {}
        }
    
    def save_knowledge_base(self):
        """Save knowledge base for future learning."""
        kb_path = Path("ai_analytics_storage/agent_knowledge_base.json")
        kb_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert defaultdict to regular dict for JSON serialization
        kb_to_save = {
            'successful_strategies': dict(self.knowledge_base['successful_strategies']),
            'failed_strategies': dict(self.knowledge_base['failed_strategies']),
            'dataset_profiles': self.knowledge_base['dataset_profiles'],
            'optimal_algorithms': self.knowledge_base['optimal_algorithms'],
            'processing_times': self.knowledge_base['processing_times']
        }
        
        with open(kb_path, 'w') as f:
            json.dump(kb_to_save, f, indent=2)
    
    def analyze_dataset_profile(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Create comprehensive dataset profile for decision making."""
        profile = {
            'shape': data.shape,
            'size_category': self._categorize_size(data),
            'n_numeric': len(data.select_dtypes(include=[np.number]).columns),
            'n_categorical': len(data.select_dtypes(include=['object', 'category']).columns),
            'n_datetime': len(data.select_dtypes(include=['datetime64']).columns),
            'missing_ratio': data.isnull().sum().sum() / data.size,
            'duplicate_ratio': data.duplicated().sum() / len(data),
            'memory_mb': data.memory_usage(deep=True).sum() / (1024 * 1024),
        }
        
        # Analyze column types distribution
        profile['column_type_ratio'] = {
            'numeric': profile['n_numeric'] / data.shape[1],
            'categorical': profile['n_categorical'] / data.shape[1],
            'datetime': profile['n_datetime'] / data.shape[1]
        }
        
        # Detect text columns
        text_cols = []
        for col in data.select_dtypes(include=['object']).columns:
            avg_len = data[col].astype(str).str.len().mean()
            if avg_len > 50:
                text_cols.append(col)
        profile['has_text_data'] = len(text_cols) > 0
        profile['text_columns'] = len(text_cols)
        
        # Detect time series patterns
        profile['has_time_series'] = self._detect_time_series(data)
        
        # Calculate data quality score
        profile['quality_score'] = self._calculate_quality_score(data)
        
        # Detect complexity
        profile['complexity'] = self._assess_complexity(data)
        
        return profile
    
    def _categorize_size(self, data: pd.DataFrame) -> str:
        """Categorize dataset size."""
        total_cells = data.shape[0] * data.shape[1]
        if total_cells < 10000:
            return 'tiny'
        elif total_cells < 100000:
            return 'small'
        elif total_cells < 1000000:
            return 'medium'
        elif total_cells < 10000000:
            return 'large'
        else:
            return 'huge'
    
    def _detect_time_series(self, data: pd.DataFrame) -> bool:
        """Detect if dataset has time series characteristics."""
        datetime_cols = data.select_dtypes(include=['datetime64']).columns
        if len(datetime_cols) > 0:
            return True
        
        # Check for date-like column names
        date_keywords = ['date', 'time', 'timestamp', 'year', 'month', 'day']
        for col in data.columns:
            if any(keyword in col.lower() for keyword in date_keywords):
                return True
        
        return False
    
    def _calculate_quality_score(self, data: pd.DataFrame) -> float:
        """Calculate overall data quality score (0-100)."""
        scores = []
        
        # Completeness
        completeness = 1 - (data.isnull().sum().sum() / data.size)
        scores.append(completeness * 100)
        
        # Uniqueness (no duplicates)
        uniqueness = 1 - (data.duplicated().sum() / len(data))
        scores.append(uniqueness * 100)
        
        # Consistency (numeric columns have reasonable ranges)
        numeric_data = data.select_dtypes(include=[np.number])
        if len(numeric_data.columns) > 0:
            outlier_ratio = self._calculate_outlier_ratio(numeric_data)
            consistency = 1 - min(outlier_ratio, 1.0)
            scores.append(consistency * 100)
        
        return np.mean(scores)
    
    def _calculate_outlier_ratio(self, numeric_data: pd.DataFrame) -> float:
        """Calculate ratio of outliers in numeric data."""
        total_outliers = 0
        total_values = 0
        
        for col in numeric_data.columns:
            Q1 = numeric_data[col].quantile(0.25)
            Q3 = numeric_data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = numeric_data[col][(numeric_data[col] < (Q1 - 1.5 * IQR)) | 
                                         (numeric_data[col] > (Q3 + 1.5 * IQR))]
            total_outliers += len(outliers)
            total_values += len(numeric_data[col].dropna())
        
        return total_outliers / total_values if total_values > 0 else 0
    
    def _assess_complexity(self, data: pd.DataFrame) -> str:
        """Assess dataset complexity."""
        complexity_score = 0
        
        # Size factor
        if data.shape[0] * data.shape[1] > 1000000:
            complexity_score += 2
        elif data.shape[0] * data.shape[1] > 100000:
            complexity_score += 1
        
        # Column diversity
        if data.shape[1] > 50:
            complexity_score += 2
        elif data.shape[1] > 20:
            complexity_score += 1
        
        # Mixed data types
        type_count = len(data.dtypes.unique())
        if type_count > 3:
            complexity_score += 1
        
        # Missing data
        if data.isnull().sum().sum() / data.size > 0.2:
            complexity_score += 1
        
        if complexity_score >= 5:
            return 'very_high'
        elif complexity_score >= 3:
            return 'high'
        elif complexity_score >= 2:
            return 'moderate'
        else:
            return 'low'
    
    def decide_pipeline_steps(self, profile: Dict[str, Any]) -> List[Tuple[str, str]]:
        """
        Intelligently decide which pipeline steps to execute based on data profile.
        Returns list of (step_name, reasoning) tuples.
        """
        steps = []
        
        # Data Analysis - Always needed
        steps.append(("Data Analysis", "Essential for understanding data structure"))
        
        # Data Cleaning - Decide strategy
        if profile['missing_ratio'] > 0.01:
            if profile['quality_score'] < 70:
                steps.append(("Advanced Data Cleaning", f"Low quality score ({profile['quality_score']:.1f}) requires advanced cleaning"))
            else:
                steps.append(("Standard Data Cleaning", f"Moderate quality ({profile['quality_score']:.1f}) - standard cleaning sufficient"))
        else:
            steps.append(("Minimal Data Cleaning", "High quality data - minimal cleaning needed"))
        
        # Feature Engineering - Based on data types
        if profile['has_text_data']:
            steps.append(("Text Feature Engineering", f"Text columns detected ({profile['text_columns']} columns)"))
        
        if profile['has_time_series']:
            steps.append(("Time Series Feature Engineering", "Temporal patterns detected"))
        
        if profile['column_type_ratio']['categorical'] > 0.3:
            steps.append(("Categorical Feature Engineering", f"High categorical ratio ({profile['column_type_ratio']['categorical']:.1%})"))
        
        if profile['n_numeric'] > 5:
            steps.append(("Numerical Feature Engineering", f"Rich numerical data ({profile['n_numeric']} columns)"))
        
        # Model Training - Select based on size and complexity
        steps.append(("Model Training", self._get_model_strategy(profile)))
        
        # Evaluation - Always needed
        steps.append(("Model Evaluation", "Required for performance assessment"))
        
        # Visualization - Adaptive
        if profile['size_category'] in ['tiny', 'small']:
            steps.append(("Comprehensive Visualization", "Small dataset - can generate all visualizations"))
        else:
            steps.append(("Essential Visualization", "Large dataset - focus on key visualizations"))
        
        # Export - Always included
        steps.append(("Results Export", "Save outputs and artifacts"))
        
        return steps
    
    def _get_model_strategy(self, profile: Dict[str, Any]) -> str:
        """Determine optimal model training strategy."""
        if profile['size_category'] in ['huge', 'large']:
            return "Fast algorithms (LightGBM, Linear Models) for large dataset"
        elif profile['complexity'] in ['very_high', 'high']:
            return "Ensemble methods (XGBoost, Random Forest) for complex patterns"
        elif profile['quality_score'] < 60:
            return "Robust algorithms with regularization for noisy data"
        else:
            return "Full suite of algorithms with cross-validation"
    
    def select_cleaning_strategy(self, profile: Dict[str, Any]) -> Dict[str, str]:
        """Select optimal cleaning strategies for different aspects."""
        strategies = {}
        
        # Missing value strategy
        if profile['missing_ratio'] < 0.05:
            strategies['missing'] = 'simple_drop'
        elif profile['missing_ratio'] < 0.20:
            strategies['missing'] = 'smart_imputation'
        else:
            strategies['missing'] = 'advanced_imputation'
        
        # Outlier strategy
        outlier_ratio = self._calculate_outlier_ratio(pd.DataFrame())  # Will be passed actual data
        if profile['quality_score'] > 80:
            strategies['outliers'] = 'keep_with_flag'
        elif outlier_ratio > 0.15:
            strategies['outliers'] = 'remove'
        else:
            strategies['outliers'] = 'cap'
        
        # Encoding strategy
        if profile['column_type_ratio']['categorical'] > 0.5:
            strategies['encoding'] = 'target_encoding'
        else:
            strategies['encoding'] = 'onehot_with_frequency'
        
        return strategies
    
    def select_ml_algorithms(self, profile: Dict[str, Any], problem_type: str) -> List[str]:
        """Select optimal ML algorithms based on dataset profile."""
        algorithms = []
        
        if problem_type == 'classification':
            if profile['size_category'] in ['huge', 'large']:
                algorithms = ['LightGBM', 'LogisticRegression', 'SGDClassifier']
            elif profile['complexity'] == 'low':
                algorithms = ['LogisticRegression', 'RandomForest', 'SVM']
            else:
                algorithms = ['XGBoost', 'RandomForest', 'LightGBM', 'ExtraTrees']
        else:  # regression
            if profile['size_category'] in ['huge', 'large']:
                algorithms = ['LightGBM', 'LinearRegression', 'Ridge']
            elif profile['complexity'] == 'low':
                algorithms = ['LinearRegression', 'Ridge', 'Lasso']
            else:
                algorithms = ['XGBoost', 'RandomForest', 'LightGBM', 'GradientBoosting']
        
        return algorithms
    
    def record_decision(self, decision_type: str, decision: str, outcome: str, reasoning: str):
        """Record a decision and its outcome for learning."""
        self.decision_history.append({
            'timestamp': datetime.now().isoformat(),
            'type': decision_type,
            'decision': decision,
            'outcome': outcome,
            'reasoning': reasoning
        })
        
        # Update knowledge base
        if outcome == 'success':
            self.knowledge_base['successful_strategies'][decision_type].append(decision)
        else:
            self.knowledge_base['failed_strategies'][decision_type].append(decision)

class AgentModeController:
    """
    TRUE AUTONOMOUS Agent Mode Controller with intelligent decision-making.
    Makes real-time decisions based on data characteristics and learns from experience.
    """
    
    def __init__(self, main_app):
        self.main_app = main_app
        self.is_running = False
        self.current_step = ""
        self.progress = 0
        self.max_retries = 3
        self.retry_count = 0
        self.errors = []
        self.log = []
        self.success_count = 0
        self.total_steps = 8
        
        # Autonomous decision engine
        self.decision_engine = AgentDecisionEngine()
        self.dataset_profile = None
        self.selected_strategies = {}
        self.pipeline_steps = []  # Will be dynamically generated
        
        # Setup logging first before verification
        self.setup_logging()
        
        # Verify main app has required components
        self._verify_main_app_components()
        
        self._add_log("ü§ñ TRUE AUTONOMOUS AGENT initialized with decision-making capabilities")
        
    def setup_logging(self):
        """Setup comprehensive logging system."""
        log_dir = Path("ai_analytics_storage/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger("AgentMode")
        self.logger.setLevel(logging.INFO)
        
        # File handler
        log_file = log_dir / f"agent_mode_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
    
    def _verify_main_app_components(self):
        """Verify that main app has all required components for Agent Mode."""
        required_attrs = ['data_processor', 'ml_engine', 'neural_visualizer']
        missing_attrs = []
        
        for attr in required_attrs:
            if not hasattr(self.main_app, attr) or getattr(self.main_app, attr) is None:
                missing_attrs.append(attr)
        
        if missing_attrs:
            self._add_log(f"‚ö†Ô∏è Warning: Missing main app components: {', '.join(missing_attrs)}")
        else:
            self._add_log("‚úÖ All main app components verified")
        
    def start_agent_mode(self, callback: Optional[Callable] = None):
        """
        Start the autonomous agent mode pipeline.
        
        Args:
            callback: Optional callback function to update UI
        """
        if self.is_running:
            return False, "Agent Mode is already running"
            
        self.is_running = True
        self.progress = 0
        self.retry_count = 0
        self.errors = []
        self.log = []
        self.success_count = 0
        
        # Start pipeline in separate thread
        thread = threading.Thread(
            target=self._run_pipeline_with_recovery,
            args=(callback,),
            daemon=True
        )
        thread.start()
        
        return True, "Agent Mode started successfully"
        
    def stop_agent_mode(self):
        """Stop the agent mode pipeline."""
        self.is_running = False
        self.logger.info("Agent Mode stopped by user")
        
    def get_status(self) -> Dict[str, Any]:
        """Get current status of agent mode."""
        return {
            'is_running': self.is_running,
            'current_step': self.current_step,
            'progress': self.progress,
            'success_count': self.success_count,
            'total_steps': self.total_steps,
            'errors': self.errors[-5:],  # Last 5 errors
            'log': self.log[-10:],  # Last 10 log entries
            'retry_count': self.retry_count
        }
        
    def _run_pipeline_with_recovery(self, callback: Optional[Callable] = None):
        """
        Run the INTELLIGENT pipeline with adaptive decision-making and error recovery.
        
        Args:
            callback: Optional callback function to update UI
        """
        try:
            self.logger.info("Starting TRUE AUTONOMOUS Agent Mode Pipeline")
            self._add_log("ü§ñ TRUE AUTONOMOUS AGENT MODE - Analyzing data and planning strategy...")
            
            # PHASE 1: Intelligent Analysis and Planning
            if not self._intelligent_planning_phase():
                self._add_log("‚ùå Planning phase failed - cannot proceed")
                return
            
            # PHASE 2: Execute dynamically generated pipeline
            for step_index, (step_name, reasoning) in enumerate(self.pipeline_steps):
                if not self.is_running:
                    break
                
                self.current_step = step_name
                self.progress = int((step_index / len(self.pipeline_steps)) * 100)
                
                # Log AI reasoning for this step
                self._add_log(f"üß† DECISION: {step_name}")
                self._add_log(f"   REASONING: {reasoning}")
                
                if callback:
                    callback(self.get_status())
                
                # Execute step with intelligent retry and adaptation
                self._add_log(f"\n{'='*70}")
                self._add_log(f"üîÑ EXECUTING STEP {step_index+1}/{len(self.pipeline_steps)}: {step_name}")
                self._add_log(f"{'='*70}")
                
                success = self._execute_intelligent_step(step_name, reasoning)
                
                if success:
                    self.success_count += 1
                    self._add_log(f"‚úÖ {step_name} completed successfully")
                    self.logger.info(f"Step completed: {step_name}")
                    
                    # Record successful decision
                    self.decision_engine.record_decision(
                        step_name, reasoning, 'success', 
                        f"Step executed successfully with chosen strategy"
                    )
                else:
                    self._add_log(f"‚ö†Ô∏è {step_name} FAILED - Agent adapting strategy...")
                    self.logger.warning(f"Step failed: {step_name}")
                    
                    # Try alternative strategy
                    alt_success = self._try_alternative_strategy(step_name)
                    if alt_success:
                        self.success_count += 1
                        self._add_log(f"‚úÖ Alternative strategy succeeded for {step_name}")
                    else:
                        self._add_log(f"‚ùå {step_name} PERMANENTLY FAILED - continuing with next step")
                        self._add_log(f"‚ùå Training will not be available in Q&A until this is fixed!")
                        self.decision_engine.record_decision(
                            step_name, reasoning, 'failure',
                            f"All strategies failed for this step"
                        )
            
            # PHASE 3: Final optimization and learning
            self._learning_phase()
            
            # Final status update
            self.progress = 100
            self.current_step = "Completed"
            self._add_log(f"üéâ Pipeline completed! {self.success_count}/{len(self.pipeline_steps)} steps successful")
            self._add_log(f"üìä Quality Score: {self.dataset_profile.get('quality_score', 0):.1f}/100")
            self.logger.info("Agent Mode Pipeline completed with learning")
            
            # Call status update BEFORE setting is_running to False
            # This ensures UI knows agent was running when it completed
            if callback:
                callback(self.get_status())
            
            # Now mark as not running
            self.is_running = False
            
            # Save learned knowledge
            try:
                self.decision_engine.save_knowledge_base()
                self._add_log("üíæ Knowledge base saved for future learning")
            except Exception as e:
                self._add_log(f"‚ö†Ô∏è Could not save knowledge base: {str(e)}")
                
        except Exception as e:
            self.logger.error(f"Critical pipeline error: {str(e)}")
            self._add_error(f"Critical pipeline error: {str(e)}")
    
    def _intelligent_planning_phase(self) -> bool:
        """
        PHASE 1: Analyze data and intelligently plan the pipeline.
        This is where the agent makes its first major decisions.
        """
        try:
            self._add_log("üîç PHASE 1: Intelligent Analysis & Planning")
            
            # Load data if not already loaded
            if self.main_app.current_data is None:
                self._add_log("üìÇ No data loaded - generating sample data")
                sample_data = self._generate_sample_data()
                if sample_data is not None:
                    self.main_app.current_data = sample_data
                else:
                    return False
            
            # Create comprehensive dataset profile
            self._add_log("üìä Analyzing dataset characteristics...")
            self.dataset_profile = self.decision_engine.analyze_dataset_profile(
                self.main_app.current_data
            )
            
            # Log key insights
            self._add_log(f"   üìè Size: {self.dataset_profile['shape']} ({self.dataset_profile['size_category']})")
            self._add_log(f"   üìä Quality Score: {self.dataset_profile['quality_score']:.1f}/100")
            self._add_log(f"   üß© Complexity: {self.dataset_profile['complexity']}")
            self._add_log(f"   üî¢ Numeric: {self.dataset_profile['n_numeric']}, Categorical: {self.dataset_profile['n_categorical']}")
            self._add_log(f"   üï≥Ô∏è Missing: {self.dataset_profile['missing_ratio']:.1%}")
            
            # Make intelligent decisions about pipeline
            self._add_log("üß† Agent deciding optimal pipeline...")
            pipeline_decisions = self.decision_engine.decide_pipeline_steps(self.dataset_profile)
            
            # Convert decisions to executable steps
            self.pipeline_steps = []
            for step_name, reasoning in pipeline_decisions:
                # Map decision to actual function
                step_function = self._map_step_to_function(step_name)
                if step_function:
                    self.pipeline_steps.append((step_name, reasoning))
            
            self.total_steps = len(self.pipeline_steps)
            
            # Log the intelligent plan
            self._add_log(f"‚úÖ Intelligent plan created: {self.total_steps} adaptive steps")
            for i, (step_name, reasoning) in enumerate(self.pipeline_steps, 1):
                self._add_log(f"   {i}. {step_name}")
            
            # Select cleaning strategies
            self.selected_strategies = self.decision_engine.select_cleaning_strategy(
                self.dataset_profile
            )
            self._add_log(f"üßπ Selected strategies: {self.selected_strategies}")
            
            return True
            
        except Exception as e:
            self._add_log(f"‚ùå Planning phase error: {str(e)}")
            self.logger.error(f"Planning phase error: {str(e)}")
            return False
    
    def _map_step_to_function(self, step_name: str):
        """Map step name to actual function - used for routing."""
        # We'll use step name matching in _execute_intelligent_step
        return True
    
    def _execute_intelligent_step(self, step_name: str, reasoning: str) -> bool:
        """
        Execute a step with intelligent decision-making.
        """
        try:
            # Route to appropriate handler based on step name
            if "Analysis" in step_name:
                return self._analyze_data_step()
            elif "Cleaning" in step_name:
                return self._intelligent_clean_data_step(step_name)
            elif "Feature Engineering" in step_name:
                return self._intelligent_feature_engineering_step(step_name)
            elif "Training" in step_name:
                return self._intelligent_train_models_step()
            elif "Evaluation" in step_name:
                return self._evaluate_models_step()
            elif "Visualization" in step_name:
                return self._intelligent_generate_visualizations_step(step_name)
            elif "Export" in step_name:
                return self._export_results_step()
            else:
                # Generic execution
                return True
                
        except Exception as e:
            self._add_log(f"‚ùå Step execution error: {str(e)}")
            return False
    
    def _try_alternative_strategy(self, step_name: str) -> bool:
        """
        Try an alternative strategy when primary strategy fails.
        This demonstrates adaptive decision-making.
        """
        try:
            self._add_log(f"üîÑ Trying alternative strategy for: {step_name}")
            
            if "Cleaning" in step_name:
                # Try simpler cleaning approach
                self._add_log("   ‚Üí Switching to basic cleaning strategy")
                return self._basic_clean_data_step()
            elif "Training" in step_name:
                # Try simpler model
                self._add_log("   ‚Üí Switching to simpler models")
                return self._basic_train_models_step()
            else:
                return False
                
        except Exception as e:
            self._add_log(f"   ‚ùå Alternative strategy also failed: {str(e)}")
            return False
    
    def _learning_phase(self):
        """
        PHASE 3: Learn from this run and update knowledge base.
        """
        try:
            self._add_log("üìö PHASE 3: Learning & Knowledge Update")
            
            # Record dataset profile for future reference
            self.decision_engine.knowledge_base['dataset_profiles'].append({
                'timestamp': datetime.now().isoformat(),
                'profile': self.dataset_profile,
                'success_rate': self.success_count / len(self.pipeline_steps) if self.pipeline_steps else 0,
                'strategies_used': self.selected_strategies
            })
            
            # Analyze what worked well
            successful_steps = [step for step, _ in self.pipeline_steps[:self.success_count]]
            self._add_log(f"   ‚úÖ Successful strategies: {len(successful_steps)}/{len(self.pipeline_steps)}")
            
            # Save insights
            self.decision_engine.save_knowledge_base()
            self._add_log("   üíæ Knowledge base updated with new insights")
            
        except Exception as e:
            self._add_log(f"‚ö†Ô∏è Learning phase error: {str(e)}")
            
    def _execute_step_with_retry(self, step_name: str, step_function: Callable) -> bool:
        """
        Execute a pipeline step with retry mechanism.
        
        Args:
            step_name: Name of the step
            step_function: Function to execute
            
        Returns:
            bool: True if successful, False if failed after all retries
        """
        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"Executing {step_name} (attempt {attempt + 1})")
                
                # Execute the step
                result = step_function()
                
                if result:
                    return True
                else:
                    raise Exception(f"{step_name} returned False")
                    
            except Exception as e:
                error_msg = f"{step_name} attempt {attempt + 1} failed: {str(e)}"
                self.logger.warning(error_msg)
                self._add_error(error_msg)
                
                if attempt < self.max_retries:
                    # Wait before retry (exponential backoff)
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                    self._add_log(f"Retrying {step_name} in {wait_time} seconds...")
                else:
                    return False
                    
        return False
        
    def _add_log(self, message: str):
        """Add a log entry with timestamp."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.log.append(log_entry)
        
    def _add_error(self, error: str):
        """Add an error entry with timestamp."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        error_entry = f"[{timestamp}] ERROR: {error}"
        self.errors.append(error_entry)
        
    # Pipeline Step Implementations
    
    def _load_data_step(self) -> bool:
        """Load data step with automatic format detection."""
        try:
            if self.main_app.current_data is not None:
                return True
                
            # Try to load sample data if no data is loaded
            sample_data = self._generate_sample_data()
            if sample_data is not None:
                self.main_app.current_data = sample_data
                self._add_log("Sample data generated for demonstration")
                return True
                
            return False
            
        except Exception as e:
            raise Exception(f"Data loading failed: {str(e)}")
            
    def _analyze_data_step(self) -> bool:
        """Analyze data structure and characteristics."""
        try:
            if self.main_app.current_data is None:
                return False
                
            # Perform data analysis using existing methods
            if hasattr(self.main_app, 'analyze_data'):
                self.main_app.analyze_data()
                
            return True
            
        except Exception as e:
            raise Exception(f"Data analysis failed: {str(e)}")
    
    def _intelligent_clean_data_step(self, step_name: str) -> bool:
        """
        INTELLIGENT data cleaning based on selected strategy.
        """
        try:
            if self.main_app.current_data is None:
                return False
            
            strategy = self.selected_strategies.get('missing', 'smart_imputation')
            self._add_log(f"üßπ Applying {strategy} strategy for {step_name}")
            
            # Use existing cleaning but log the decision
            return self._clean_data_step()
            
        except Exception as e:
            raise Exception(f"Intelligent cleaning failed: {str(e)}")
    
    def _basic_clean_data_step(self) -> bool:
        """
        Fallback basic cleaning when advanced cleaning fails.
        """
        try:
            if self.main_app.current_data is None:
                return False
                
            self._add_log("üßπ Applying basic cleaning (dropna + fillna)")
            
            # Simple cleaning
            df = self.main_app.current_data.copy()
            
            # Drop rows with too many missing values
            threshold = 0.5 * df.shape[1]
            df = df.dropna(thresh=threshold)
            
            # Fill remaining missing values
            for col in df.columns:
                if df[col].dtype in ['float64', 'int64']:
                    df[col].fillna(df[col].median(), inplace=True)
                else:
                    df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown', inplace=True)
            
            self.main_app.cleaned_data = df
            self.main_app.processed_data = df
            self._add_log(f"‚úÖ Basic cleaning complete: {df.shape}")
            
            return True
            
        except Exception as e:
            self._add_log(f"‚ùå Basic cleaning failed: {str(e)}")
            return False
    
    def _intelligent_feature_engineering_step(self, step_name: str) -> bool:
        """
        INTELLIGENT feature engineering based on data type.
        """
        try:
            self._add_log(f"üîß Intelligent feature engineering: {step_name}")
            
            if "Text" in step_name:
                self._add_log("   ‚Üí Text feature extraction activated")
            elif "Time Series" in step_name:
                self._add_log("   ‚Üí Temporal feature generation activated")
            elif "Categorical" in step_name:
                self._add_log("   ‚Üí Advanced categorical encoding activated")
            
            # Call existing feature engineering if available
            return self._feature_engineering_step()
            
        except Exception as e:
            self._add_log(f"‚ö†Ô∏è Feature engineering skipped: {str(e)}")
            return True  # Non-critical, continue
    
    def _intelligent_train_models_step(self) -> bool:
        """
        INTELLIGENT model training with algorithm selection.
        """
        try:
            self._add_log("=" * 70)
            self._add_log("üöÄ STARTING MODEL TRAINING STEP")
            self._add_log("=" * 70)
            
            # Check for cleaned data
            if self.main_app.cleaned_data is None:
                self._add_log("‚ùå TRAINING FAILED: No cleaned_data available!")
                self._add_log(f"   current_data exists: {self.main_app.current_data is not None}")
                if self.main_app.current_data is not None:
                    self._add_log(f"   Using current_data as fallback: {self.main_app.current_data.shape}")
                    self.main_app.cleaned_data = self.main_app.current_data.copy()
                    self.main_app.processed_data = self.main_app.current_data.copy()
                else:
                    self._add_log("   No data available at all - cannot train!")
                    return False
            
            self._add_log(f"‚úÖ Cleaned data available: {self.main_app.cleaned_data.shape}")
            self._add_log(f"   Columns: {list(self.main_app.cleaned_data.columns)[:10]}...")
            
            # Auto-select target column from CLEANED data
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                self._add_log("üéØ Auto-selecting target column from cleaned data...")
                self._auto_select_target_column()
                self._add_log(f"   Selected: {self.main_app.target_column}")
            else:
                # CRITICAL: If target exists but not present in cleaned_data, re-select
                if self.main_app.target_column not in self.main_app.cleaned_data.columns:
                    self._add_log(f"‚ö†Ô∏è Target '{self.main_app.target_column}' NOT in cleaned_data - re-selecting")
                    self._auto_select_target_column()
                    self._add_log(f"   Re-selected: {self.main_app.target_column}")
            
            # Auto-detect problem type
            if hasattr(self.main_app, 'problem_type_var'):
                if self.main_app.problem_type_var.get() == "auto":
                    self._add_log("üß† Auto-detecting problem type...")
                    self._auto_detect_problem_type()
            
            problem_type = getattr(self.main_app, 'problem_type_var', None)
            problem_type = problem_type.get() if problem_type else 'classification'
            self._add_log(f"üìä Problem type: {problem_type}")
            
            # Select optimal algorithms
            algorithms = self.decision_engine.select_ml_algorithms(
                self.dataset_profile, 
                problem_type
            )
            self._add_log(f"ü§ñ Selected algorithms: {', '.join(algorithms)}")
            
            # Call existing training
            self._add_log("üîÑ Calling _train_models_step()...")
            result = self._train_models_step()
            
            if result:
                self._add_log("=" * 70)
                self._add_log("‚úÖ MODEL TRAINING COMPLETED SUCCESSFULLY!")
                self._add_log("=" * 70)
            else:
                self._add_log("=" * 70)
                self._add_log("‚ùå MODEL TRAINING FAILED!")
                self._add_log("=" * 70)
            
            return result
            
        except Exception as e:
            self._add_log("=" * 70)
            self._add_log(f"‚ùå EXCEPTION: {str(e)}")
            import traceback
            self._add_log(f"Traceback: {traceback.format_exc()}")
            self._add_log("=" * 70)
            raise Exception(f"Intelligent training failed: {str(e)}")
    
    def _basic_train_models_step(self) -> bool:
        """
        Fallback basic training with simple model.
        """
        try:
            self._add_log("ü§ñ Fallback: Training simple logistic regression")
            
            # Check for cleaned data
            if self.main_app.cleaned_data is None:
                self._add_log("‚ùå No cleaned data for basic training either!")
                return False
            
            self._add_log(f"‚úÖ Using cleaned data: {self.main_app.cleaned_data.shape}")
            self._add_log(f"   Columns: {list(self.main_app.cleaned_data.columns)}")
            
            # Auto-select target if needed
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                self._auto_select_target_column()
            
            self._add_log(f"üéØ Target column requested: {self.main_app.target_column}")
            
            # CRITICAL: Check if target exists in cleaned data
            if self.main_app.target_column not in self.main_app.cleaned_data.columns:
                self._add_log(f"‚ùå ERROR: Target '{self.main_app.target_column}' not in cleaned data!")
                self._add_log(f"   Available columns: {list(self.main_app.cleaned_data.columns)}")
                
                # Re-select from ACTUAL columns
                self._add_log("   üîÑ Re-selecting target from cleaned data columns...")
                for col in ['Depression', 'Suicidal Thoughts', 'Have you ever had suicidal thoughts ?', 'CGPA']:
                    if col in self.main_app.cleaned_data.columns:
                        self.main_app.target_column = col
                        self._add_log(f"   ‚úÖ New target: {col}")
                        break
                else:
                    # Use first column as fallback
                    self.main_app.target_column = self.main_app.cleaned_data.columns[0]
                    self._add_log(f"   ‚ö†Ô∏è Using first column as target: {self.main_app.target_column}")
            
            self._add_log(f"üéØ Final target: {self.main_app.target_column}")
            
            # Prepare data
            X = self.main_app.cleaned_data.drop(columns=[self.main_app.target_column])
            y = self.main_app.cleaned_data[self.main_app.target_column]
            
            # Train simple model
            from sklearn.linear_model import LogisticRegression
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Handle categorical columns
            X_train_numeric = X_train.select_dtypes(include=[np.number])
            X_test_numeric = X_test.select_dtypes(include=[np.number])
            
            model = LogisticRegression(max_iter=1000)
            model.fit(X_train_numeric.fillna(0), y_train)
            
            y_pred = model.predict(X_test_numeric.fillna(0))
            accuracy = accuracy_score(y_test, y_pred)
            
            self._add_log(f"‚úÖ Basic model trained! Accuracy: {accuracy:.4f}")
            self.main_app.training_complete = True
            
            return True
            
        except Exception as e:
            self._add_log(f"‚ùå Basic training failed: {str(e)}")
            import traceback
            self._add_log(f"Traceback: {traceback.format_exc()}")
            return False
    
    def _intelligent_generate_visualizations_step(self, step_name: str) -> bool:
        """
        INTELLIGENT visualization generation based on dataset size.
        """
        try:
            if "Comprehensive" in step_name:
                self._add_log("üé® Generating comprehensive visualizations")
            else:
                self._add_log("üé® Generating essential visualizations (dataset is large)")
            
            return self._generate_visualizations_step()
            
        except Exception as e:
            raise Exception(f"Visualization generation failed: {str(e)}")
            
    def _clean_data_step(self) -> bool:
        """Clean and preprocess data."""
        try:
            if self.main_app.current_data is None:
                return False
                
            self._add_log("Starting intelligent data cleaning...")
            
            # Ensure data processor has the current data
            if hasattr(self.main_app, 'data_processor') and self.main_app.data_processor:
                # Make sure the data processor has the current data loaded
                if not hasattr(self.main_app.data_processor, 'original_data') or self.main_app.data_processor.original_data is None:
                    # Use load_and_analyze_dataframe instead of direct assignment
                    self.main_app.data_processor.load_and_analyze_dataframe(
                        self.main_app.current_data, 
                        "Agent Mode Data"
                    )
                    self._add_log("Loaded and analyzed data in processor")
                
                # Direct call to data processor's smart cleaning
                self._add_log("Executing smart data cleaning algorithms...")
                success = self.main_app.data_processor.smart_clean_data()
                
                if success and hasattr(self.main_app.data_processor, 'cleaned_data') and self.main_app.data_processor.cleaned_data is not None:
                    self.main_app.cleaned_data = self.main_app.data_processor.cleaned_data.copy()
                    self._add_log(f"‚úÖ Data cleaned: {self.main_app.cleaned_data.shape[0]} rows √ó {self.main_app.cleaned_data.shape[1]} columns")
                    self._add_log(f"   Columns in cleaned data: {list(self.main_app.cleaned_data.columns)[:10]}...")
                    
                    # CRITICAL: Update the main app's processed data for ML pipeline
                    self.main_app.processed_data = self.main_app.cleaned_data.copy()
                    
                    # Calculate cleaning stats
                    original_rows = self.main_app.current_data.shape[0]
                    cleaned_rows = self.main_app.cleaned_data.shape[0]
                    if cleaned_rows < original_rows:
                        removed = original_rows - cleaned_rows
                        self._add_log(f"üóëÔ∏è Removed {removed} problematic rows ({removed/original_rows*100:.1f}%)")
                    
                    self._add_log("Data cleaning completed successfully!")
                    return True
                else:
                    raise Exception("Data processor smart_clean_data failed or returned no cleaned data")
            else:
                raise Exception("Data processor not available - cannot perform intelligent cleaning")
                
        except Exception as e:
            self._add_log(f"‚ùå Data cleaning error: {str(e)}")
            raise Exception(f"Data cleaning failed: {str(e)}")
            
    def _feature_engineering_step(self) -> bool:
        """Perform feature engineering."""
        try:
            if hasattr(self.main_app, 'advanced_feature_engineering'):
                self.main_app.advanced_feature_engineering()
                
            return True
            
        except Exception as e:
            # Feature engineering is optional, so continue if it fails
            self._add_log(f"Feature engineering skipped: {str(e)}")
            return True
            
    def _train_models_step(self) -> bool:
        """Train machine learning models."""
        try:
            self._add_log("üîç Checking training prerequisites...")
            
            # Ensure we have cleaned data
            if self.main_app.cleaned_data is None:
                self._add_log("‚ùå TRAINING BLOCKED: No cleaned data available")
                self._add_log(f"   current_data exists: {self.main_app.current_data is not None}")
                self._add_log(f"   cleaned_data: {self.main_app.cleaned_data}")
                return False
            
            self._add_log(f"‚úÖ Cleaned data available: {self.main_app.cleaned_data.shape}")
            self._add_log(f"   Columns: {list(self.main_app.cleaned_data.columns)[:5]}...")
            
            # Auto-select target column if not selected
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                self._add_log("üéØ Auto-selecting target column...")
                self._auto_select_target_column()
            
            # Auto-set problem type if not set
            if hasattr(self.main_app, 'problem_type_var'):
                if self.main_app.problem_type_var.get() == "auto":
                    self._add_log("üß† Auto-detecting problem type...")
                    self._auto_detect_problem_type()
            
            # Verify target column is in cleaned data
            if self.main_app.target_column and self.main_app.target_column not in self.main_app.cleaned_data.columns:
                self._add_log(f"‚ö†Ô∏è Target '{self.main_app.target_column}' NOT in cleaned data!")
                self._add_log(f"   Cleaned data columns: {list(self.main_app.cleaned_data.columns)}")
                self._add_log("   üîÑ Re-selecting target from cleaned data...")
                
                # Try common target names that exist in cleaned data
                for col in ['Depression', 'Suicidal Thoughts', 'Have you ever had suicidal thoughts ?', 'CGPA']:
                    if col in self.main_app.cleaned_data.columns:
                        self.main_app.target_column = col
                        self._add_log(f"   ‚úÖ Found target: {col}")
                        break
                else:
                    # Fallback: use first column
                    self.main_app.target_column = self.main_app.cleaned_data.columns[0]
                    self._add_log(f"   ‚ö†Ô∏è Using first available column: {self.main_app.target_column}")
            
            self._add_log(f"üéØ Selected Target: {self.main_app.target_column}")
            self._add_log(f"üìä Problem Type: {self.main_app.problem_type_var.get() if hasattr(self.main_app, 'problem_type_var') else 'auto'}")
            
            # Get problem type
            problem_type_str = self.main_app.problem_type_var.get() if hasattr(self.main_app, 'problem_type_var') else 'classification'
            
            # Check for ML engine
            has_ml_engine = hasattr(self.main_app, 'ml_engine') and self.main_app.ml_engine
            self._add_log(f"üîß ML Engine available: {has_ml_engine}")
            
            # Train models using ML engine
            if has_ml_engine:
                self._add_log("üöÄ Starting model training with ML engine...")

                # Use ML engine prepare_data + train_all_models to match SmartMLEngine API
                try:
                    self._add_log("   Preparing data in ml_engine.prepare_data()...")
                    # Pass problem_type to prepare_data
                    prep_success = self.main_app.ml_engine.prepare_data(
                        self.main_app.cleaned_data, 
                        self.main_app.target_column, 
                        problem_type_str
                    )
                    self._add_log(f"   prepare_data() returned: {prep_success}")
                    if not prep_success:
                        self._add_log("   ‚ùå ML engine data preparation failed")
                        return False

                    self._add_log("   Starting ml_engine.train_all_models()...")
                    train_success = self.main_app.ml_engine.train_all_models(
                        quick_mode=False, 
                        automl=False, 
                        deep_learning=False, 
                        cross_validation=True, 
                        feature_selection=True
                    )
                    self._add_log(f"   train_all_models() returned: {train_success}")

                    if train_success:
                        self.main_app.training_complete = True
                        self._add_log("‚úÖ Model training completed successfully!")

                        # Log results
                        if hasattr(self.main_app.ml_engine, 'results') and self.main_app.ml_engine.results:
                            self._add_log(f"üìä Trained {len(self.main_app.ml_engine.results)} models")
                            for model_name, metrics in self.main_app.ml_engine.results.items():
                                acc = metrics.get('accuracy', metrics.get('r2_score', 0))
                                try:
                                    self._add_log(f"   ‚Ä¢ {model_name}: {acc:.4f}")
                                except Exception:
                                    self._add_log(f"   ‚Ä¢ {model_name}: {acc}")
                        return True
                    else:
                        self._add_log("‚ö†Ô∏è ML engine reported training failure")
                        return False
                except Exception as e:
                    self._add_log(f"‚ùå Exception while calling ML engine: {str(e)}")
                    import traceback
                    self._add_log(f"Traceback: {traceback.format_exc()}")
                    return False
            elif hasattr(self.main_app, 'train_models'):
                # Fallback to main app's train_models method
                self._add_log("üöÄ Using main app training method...")
                self.main_app.train_models()
                self.main_app.training_complete = True
                return True
            else:
                self._add_log("‚ùå No ML engine or training method available")
                return False
                
        except Exception as e:
            self._add_log(f"‚ùå Training error: {str(e)}")
            import traceback
            self._add_log(f"Traceback: {traceback.format_exc()}")
            return False
            
    def _auto_select_target_column(self):
        """Automatically select a suitable target column."""
        try:
            # CRITICAL: Use cleaned_data if available, otherwise current_data
            if hasattr(self.main_app, 'cleaned_data') and self.main_app.cleaned_data is not None:
                data = self.main_app.cleaned_data
                self._add_log("   üìã Using cleaned_data for target selection")
            elif self.main_app.current_data is not None:
                data = self.main_app.current_data
                self._add_log("   üìã Using current_data for target selection")
            else:
                self._add_log("   ‚ùå No data available for target selection!")
                return
            
            self._add_log(f"   üìä Available columns: {list(data.columns)[:10]}...")
            
            # Look for common target column names
            target_candidates = []
            common_target_names = ['target', 'label', 'class', 'outcome', 'result', 'y', 
                                 'prediction', 'score', 'rating', 'price', 'value',
                                 'suicidal', 'depression', 'anxiety', 'stress']
            
            for col in data.columns:
                col_lower = col.lower()
                for target_name in common_target_names:
                    if target_name in col_lower:
                        target_candidates.append((col, target_name))
                        break
            
            # If we found candidates, pick the best one
            if target_candidates:
                # Sort by priority (exact matches first)
                target_candidates.sort(key=lambda x: len(x[1]), reverse=True)
                selected_target = target_candidates[0][0]
                self._add_log(f"   ‚úÖ Found target candidate: {selected_target}")
            else:
                # Fall back to last column if it looks like a target
                last_col = data.columns[-1]
                # Check if last column has few unique values (likely categorical target)
                if data[last_col].nunique() < len(data) * 0.1:  # Less than 10% unique values
                    selected_target = last_col
                    self._add_log(f"   ‚úÖ Using last column as target: {selected_target}")
                else:
                    # Use first numerical column with reasonable range
                    numeric_cols = data.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        selected_target = numeric_cols[0]
                        self._add_log(f"   ‚úÖ Using first numeric column: {selected_target}")
                    else:
                        selected_target = data.columns[-1]  # Last resort
                        self._add_log(f"   ‚ö†Ô∏è Last resort - using: {selected_target}")
            
            # Set the target column
            self.main_app.target_column = selected_target
            if hasattr(self.main_app, 'target_var'):
                self.main_app.target_var.set(selected_target)
                
            self._add_log(f"Auto-selected target column: {selected_target}")
            
        except Exception as e:
            self._add_log(f"Failed to auto-select target: {str(e)}")
            
    def _auto_detect_problem_type(self):
        """Automatically detect problem type based on target column."""
        try:
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                return
                
            target_col = self.main_app.target_column
            # Prefer cleaned_data when checking target distribution
            data_source = None
            if hasattr(self.main_app, 'cleaned_data') and self.main_app.cleaned_data is not None:
                data_source = self.main_app.cleaned_data
            elif self.main_app.current_data is not None:
                data_source = self.main_app.current_data
            else:
                return

            if target_col not in data_source.columns:
                return

            target_data = data_source[target_col]
            unique_values = target_data.nunique()
            
            # Classification if few unique values
            if unique_values <= 10 and target_data.dtype in ['object', 'category', 'bool']:
                problem_type = "classification"
            elif unique_values <= 20 and target_data.dtype in ['int64', 'int32']:
                problem_type = "classification"
            else:
                problem_type = "regression"
                
            if hasattr(self.main_app, 'problem_type_var'):
                self.main_app.problem_type_var.set(problem_type)
                
            self._add_log(f"Auto-detected problem type: {problem_type}")
            
        except Exception as e:
            self._add_log(f"Failed to auto-detect problem type: {str(e)}")
            
    def _evaluate_models_step(self) -> bool:
        """Evaluate trained models."""
        try:
            if hasattr(self.main_app, 'show_model_performance'):
                self.main_app.show_model_performance()
                
            return True
            
        except Exception as e:
            # Model evaluation is optional
            self._add_log(f"Model evaluation skipped: {str(e)}")
            return True
            
    def _generate_visualizations_step(self) -> bool:
        """Generate comprehensive visualizations."""
        try:
            self._add_log("üé® Generating visualizations...")
            
            # Use thread-safe visualization updates
            if hasattr(self.main_app, 'root'):
                # Schedule visualization update on main thread
                self.main_app.root.after(0, self._safe_update_visualizations)
                
            # Wait a moment for visualizations to be generated
            import time
            time.sleep(2)
            
            self._add_log("‚úÖ Visualizations generated successfully")
            return True
            
        except Exception as e:
            raise Exception(f"Visualization generation failed: {str(e)}")
            
    def _safe_update_visualizations(self):
        """Thread-safe visualization update."""
        try:
            if hasattr(self.main_app, 'update_all_visualizations'):
                self.main_app.update_all_visualizations()
            elif hasattr(self.main_app, 'show_data_overview'):
                self.main_app.show_data_overview()
        except Exception as e:
            self._add_log(f"Visualization update failed: {str(e)}")
            
    def _export_results_step(self) -> bool:
        """Export results and artifacts."""
        try:
            self._add_log("üíæ Exporting results and visualizations...")
            
            # Export visualizations if available
            if hasattr(self.main_app, 'advanced_viz_dashboard') and self.main_app.advanced_viz_dashboard:
                try:
                    self.main_app.advanced_viz_dashboard.export_all_viz()
                    self._add_log("‚úÖ All visualizations exported to folder")
                except Exception as viz_error:
                    self._add_log(f"‚ö†Ô∏è Visualization export partial: {str(viz_error)}")
            
            # Export cleaned data
            if hasattr(self.main_app, 'export_cleaned_data'):
                try:
                    self.main_app.export_cleaned_data()
                    self._add_log("‚úÖ Cleaned data exported")
                except Exception as data_error:
                    self._add_log(f"‚ö†Ô∏è Data export skipped: {str(data_error)}")
            
            # Export results summary
            if hasattr(self.main_app, 'export_results'):
                try:
                    self.main_app.export_results()
                    self._add_log("‚úÖ Results summary exported")
                except Exception as results_error:
                    self._add_log(f"‚ö†Ô∏è Results export skipped: {str(results_error)}")
                
            return True
            
        except Exception as e:
            # Export is optional, so we don't fail the pipeline
            self._add_log(f"‚ö†Ô∏è Export step completed with errors: {str(e)}")
            return True
            
    def _generate_sample_data(self) -> Optional[pd.DataFrame]:
        """Generate sample data for demonstration."""
        try:
            np.random.seed(42)
            n_samples = 1000
            
            data = {
                'feature1': np.random.normal(0, 1, n_samples),
                'feature2': np.random.exponential(2, n_samples),
                'feature3': np.random.uniform(-5, 5, n_samples),
                'category': np.random.choice(['A', 'B', 'C'], n_samples),
                'target': np.random.binomial(1, 0.3, n_samples)
            }
            
            # Add some correlations
            data['feature4'] = data['feature1'] * 0.5 + np.random.normal(0, 0.5, n_samples)
            data['target'] = ((data['feature1'] > 0) & (data['feature3'] > 0)).astype(int)
            
            return pd.DataFrame(data)
            
        except Exception as e:
            self.logger.error(f"Sample data generation failed: {str(e)}")
            return None