"""
Agent Mode System - TRUE AUTONOMOUS AI Data Analysis Pipeline
Provides intelligent decision-making, adaptive strategies, and self-optimization.
Includes user instruction handling with ReAct-style reasoning.
"""

import asyncio
import threading
import time
import traceback
import logging
from datetime import datetime
from typing import Dict, List, Tuple, Optional, Any, Callable
from pathlib import Path
from collections import defaultdict
import json
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Import instruction handler
try:
    from instruction_handler import UserInstructionHandler, InstructionType
    INSTRUCTION_HANDLER_AVAILABLE = True
except ImportError:
    INSTRUCTION_HANDLER_AVAILABLE = False
    print("âš ï¸ Instruction handler not available. Running without user instruction support.")

class AgentDecisionEngine:
    """
    Intelligent decision engine that makes autonomous choices based on data characteristics.
    """
    
    def __init__(self):
        self.knowledge_base = self._load_knowledge_base()
        self.decision_history = []
        self.performance_metrics = {}
        
    def _load_knowledge_base(self) -> Dict:
        """Load or initialize knowledge base from previous runs."""
        kb_path = Path("ai_analytics_storage/agent_knowledge_base.json")
        if kb_path.exists():
            try:
                with open(kb_path, 'r') as f:
                    return json.load(f)
            except:
                pass
        
        # Default knowledge base
        return {
            'successful_strategies': defaultdict(list),
            'failed_strategies': defaultdict(list),
            'dataset_profiles': [],
            'optimal_algorithms': {},
            'processing_times': {}
        }
    
    def save_knowledge_base(self):
        """Save knowledge base for future learning."""
        kb_path = Path("ai_analytics_storage/agent_knowledge_base.json")
        kb_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Convert defaultdict to regular dict for JSON serialization
        kb_to_save = {
            'successful_strategies': dict(self.knowledge_base['successful_strategies']),
            'failed_strategies': dict(self.knowledge_base['failed_strategies']),
            'dataset_profiles': self.knowledge_base['dataset_profiles'],
            'optimal_algorithms': self.knowledge_base['optimal_algorithms'],
            'processing_times': self.knowledge_base['processing_times']
        }
        
        with open(kb_path, 'w') as f:
            json.dump(kb_to_save, f, indent=2)
    
    def analyze_dataset_profile(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Create comprehensive dataset profile for decision making."""
        profile = {
            'shape': data.shape,
            'size_category': self._categorize_size(data),
            'n_numeric': len(data.select_dtypes(include=[np.number]).columns),
            'n_categorical': len(data.select_dtypes(include=['object', 'category']).columns),
            'n_datetime': len(data.select_dtypes(include=['datetime64']).columns),
            'missing_ratio': data.isnull().sum().sum() / data.size,
            'duplicate_ratio': data.duplicated().sum() / len(data),
            'memory_mb': data.memory_usage(deep=True).sum() / (1024 * 1024),
        }
        
        # Analyze column types distribution
        profile['column_type_ratio'] = {
            'numeric': profile['n_numeric'] / data.shape[1],
            'categorical': profile['n_categorical'] / data.shape[1],
            'datetime': profile['n_datetime'] / data.shape[1]
        }
        
        # Detect text columns
        text_cols = []
        for col in data.select_dtypes(include=['object']).columns:
            avg_len = data[col].astype(str).str.len().mean()
            if avg_len > 50:
                text_cols.append(col)
        profile['has_text_data'] = len(text_cols) > 0
        profile['text_columns'] = len(text_cols)
        
        # Detect time series patterns
        profile['has_time_series'] = self._detect_time_series(data)
        
        # Calculate data quality score
        profile['quality_score'] = self._calculate_quality_score(data)
        
        # Detect complexity
        profile['complexity'] = self._assess_complexity(data)
        
        return profile
    
    def _categorize_size(self, data: pd.DataFrame) -> str:
        """Categorize dataset size."""
        total_cells = data.shape[0] * data.shape[1]
        if total_cells < 10000:
            return 'tiny'
        elif total_cells < 100000:
            return 'small'
        elif total_cells < 1000000:
            return 'medium'
        elif total_cells < 10000000:
            return 'large'
        else:
            return 'huge'
    
    def _detect_time_series(self, data: pd.DataFrame) -> bool:
        """Detect if dataset has time series characteristics."""
        datetime_cols = data.select_dtypes(include=['datetime64']).columns
        if len(datetime_cols) > 0:
            return True
        
        # Check for date-like column names
        date_keywords = ['date', 'time', 'timestamp', 'year', 'month', 'day']
        for col in data.columns:
            if any(keyword in col.lower() for keyword in date_keywords):
                return True
        
        return False
    
    def _calculate_quality_score(self, data: pd.DataFrame) -> float:
        """Calculate overall data quality score (0-100)."""
        scores = []
        
        # Completeness
        completeness = 1 - (data.isnull().sum().sum() / data.size)
        scores.append(completeness * 100)
        
        # Uniqueness (no duplicates)
        uniqueness = 1 - (data.duplicated().sum() / len(data))
        scores.append(uniqueness * 100)
        
        # Consistency (numeric columns have reasonable ranges)
        numeric_data = data.select_dtypes(include=[np.number])
        if len(numeric_data.columns) > 0:
            outlier_ratio = self._calculate_outlier_ratio(numeric_data)
            consistency = 1 - min(outlier_ratio, 1.0)
            scores.append(consistency * 100)
        
        return np.mean(scores)
    
    def _calculate_outlier_ratio(self, numeric_data: pd.DataFrame) -> float:
        """Calculate ratio of outliers in numeric data."""
        total_outliers = 0
        total_values = 0
        
        for col in numeric_data.columns:
            Q1 = numeric_data[col].quantile(0.25)
            Q3 = numeric_data[col].quantile(0.75)
            IQR = Q3 - Q1
            outliers = numeric_data[col][(numeric_data[col] < (Q1 - 1.5 * IQR)) | 
                                         (numeric_data[col] > (Q3 + 1.5 * IQR))]
            total_outliers += len(outliers)
            total_values += len(numeric_data[col].dropna())
        
        return total_outliers / total_values if total_values > 0 else 0
    
    def _assess_complexity(self, data: pd.DataFrame) -> str:
        """Assess dataset complexity."""
        complexity_score = 0
        
        # Size factor
        if data.shape[0] * data.shape[1] > 1000000:
            complexity_score += 2
        elif data.shape[0] * data.shape[1] > 100000:
            complexity_score += 1
        
        # Column diversity
        if data.shape[1] > 50:
            complexity_score += 2
        elif data.shape[1] > 20:
            complexity_score += 1
        
        # Mixed data types
        type_count = len(data.dtypes.unique())
        if type_count > 3:
            complexity_score += 1
        
        # Missing data
        if data.isnull().sum().sum() / data.size > 0.2:
            complexity_score += 1
        
        if complexity_score >= 5:
            return 'very_high'
        elif complexity_score >= 3:
            return 'high'
        elif complexity_score >= 2:
            return 'moderate'
        else:
            return 'low'
    
    def decide_pipeline_steps(self, profile: Dict[str, Any]) -> List[Tuple[str, str]]:
        """
        Intelligently decide which pipeline steps to execute based on data profile.
        Returns list of (step_name, reasoning) tuples.
        """
        steps = []
        
        # Data Analysis - Always needed
        steps.append(("Data Analysis", "Essential for understanding data structure"))
        
        # Data Cleaning - Decide strategy
        if profile['missing_ratio'] > 0.01:
            if profile['quality_score'] < 70:
                steps.append(("Advanced Data Cleaning", f"Low quality score ({profile['quality_score']:.1f}) requires advanced cleaning"))
            else:
                steps.append(("Standard Data Cleaning", f"Moderate quality ({profile['quality_score']:.1f}) - standard cleaning sufficient"))
        else:
            steps.append(("Minimal Data Cleaning", "High quality data - minimal cleaning needed"))
        
        # Feature Engineering - Based on data types
        if profile['has_text_data']:
            steps.append(("Text Feature Engineering", f"Text columns detected ({profile['text_columns']} columns)"))
        
        if profile['has_time_series']:
            steps.append(("Time Series Feature Engineering", "Temporal patterns detected"))
        
        if profile['column_type_ratio']['categorical'] > 0.3:
            steps.append(("Categorical Feature Engineering", f"High categorical ratio ({profile['column_type_ratio']['categorical']:.1%})"))
        
        if profile['n_numeric'] > 5:
            steps.append(("Numerical Feature Engineering", f"Rich numerical data ({profile['n_numeric']} columns)"))
        
        # Model Training - Select based on size and complexity
        steps.append(("Model Training", self._get_model_strategy(profile)))
        
        # Evaluation - Always needed
        steps.append(("Model Evaluation", "Required for performance assessment"))
        
        # Visualization - Adaptive
        if profile['size_category'] in ['tiny', 'small']:
            steps.append(("Comprehensive Visualization", "Small dataset - can generate all visualizations"))
        else:
            steps.append(("Essential Visualization", "Large dataset - focus on key visualizations"))
        
        # Export - Always included
        steps.append(("Results Export", "Save outputs and artifacts"))
        
        return steps
    
    def _get_model_strategy(self, profile: Dict[str, Any]) -> str:
        """Determine optimal model training strategy."""
        if profile['size_category'] in ['huge', 'large']:
            return "Fast algorithms (LightGBM, Linear Models) for large dataset"
        elif profile['complexity'] in ['very_high', 'high']:
            return "Ensemble methods (XGBoost, Random Forest) for complex patterns"
        elif profile['quality_score'] < 60:
            return "Robust algorithms with regularization for noisy data"
        else:
            return "Full suite of algorithms with cross-validation"
    
    def select_cleaning_strategy(self, profile: Dict[str, Any]) -> Dict[str, str]:
        """Select optimal cleaning strategies for different aspects."""
        strategies = {}
        
        # Missing value strategy
        if profile['missing_ratio'] < 0.05:
            strategies['missing'] = 'simple_drop'
        elif profile['missing_ratio'] < 0.20:
            strategies['missing'] = 'smart_imputation'
        else:
            strategies['missing'] = 'advanced_imputation'
        
        # Outlier strategy
        outlier_ratio = self._calculate_outlier_ratio(pd.DataFrame())  # Will be passed actual data
        if profile['quality_score'] > 80:
            strategies['outliers'] = 'keep_with_flag'
        elif outlier_ratio > 0.15:
            strategies['outliers'] = 'remove'
        else:
            strategies['outliers'] = 'cap'
        
        # Encoding strategy
        if profile['column_type_ratio']['categorical'] > 0.5:
            strategies['encoding'] = 'target_encoding'
        else:
            strategies['encoding'] = 'onehot_with_frequency'
        
        return strategies
    
    def select_ml_algorithms(self, profile: Dict[str, Any], problem_type: str) -> List[str]:
        """Select optimal ML algorithms based on dataset profile."""
        algorithms = []
        
        if problem_type == 'classification':
            if profile['size_category'] in ['huge', 'large']:
                algorithms = ['LightGBM', 'LogisticRegression', 'SGDClassifier']
            elif profile['complexity'] == 'low':
                algorithms = ['LogisticRegression', 'RandomForest', 'SVM']
            else:
                algorithms = ['XGBoost', 'RandomForest', 'LightGBM', 'ExtraTrees']
        else:  # regression
            if profile['size_category'] in ['huge', 'large']:
                algorithms = ['LightGBM', 'LinearRegression', 'Ridge']
            elif profile['complexity'] == 'low':
                algorithms = ['LinearRegression', 'Ridge', 'Lasso']
            else:
                algorithms = ['XGBoost', 'RandomForest', 'LightGBM', 'GradientBoosting']
        
        return algorithms
    
    def record_decision(self, decision_type: str, decision: str, outcome: str, reasoning: str):
        """Record a decision and its outcome for learning."""
        self.decision_history.append({
            'timestamp': datetime.now().isoformat(),
            'type': decision_type,
            'decision': decision,
            'outcome': outcome,
            'reasoning': reasoning
        })
        
        # Update knowledge base with safe access
        try:
            if outcome == 'success':
                if decision_type not in self.knowledge_base['successful_strategies']:
                    self.knowledge_base['successful_strategies'][decision_type] = []
                self.knowledge_base['successful_strategies'][decision_type].append(decision)
            else:
                if decision_type not in self.knowledge_base['failed_strategies']:
                    self.knowledge_base['failed_strategies'][decision_type] = []
                self.knowledge_base['failed_strategies'][decision_type].append(decision)
        except Exception as e:
            # Don't crash if knowledge base update fails
            pass

class AgentModeController:
    """
    TRUE AUTONOMOUS Agent Mode Controller with intelligent decision-making.
    Makes real-time decisions based on data characteristics and learns from experience.
    """
    
    def __init__(self, main_app):
        self.main_app = main_app
        self.is_running = False
        self.current_step = ""
        self.progress = 0
        self.max_retries = 3
        self.retry_count = 0
        self.errors = []
        self.log = []
        self.success_count = 0
        self.total_steps = 8
        
        # Autonomous decision engine
        self.decision_engine = AgentDecisionEngine()
        
        # User instruction handler with ReAct reasoning
        self.instruction_handler = UserInstructionHandler() if INSTRUCTION_HANDLER_AVAILABLE else None
        self.user_instructions = {}
        self.execution_plan = []
        self.dataset_profile = None
        self.selected_strategies = {}
        self.pipeline_steps = []  # Will be dynamically generated
        
        # Setup logging first before verification
        self.setup_logging()
        
        # Verify main app has required components
        self._verify_main_app_components()
        
        self._add_log("ðŸ¤– TRUE AUTONOMOUS AGENT initialized with decision-making capabilities")
        
    def setup_logging(self):
        """Setup comprehensive logging system."""
        log_dir = Path("ai_analytics_storage/logs")
        log_dir.mkdir(parents=True, exist_ok=True)
        
        self.logger = logging.getLogger("AgentMode")
        self.logger.setLevel(logging.INFO)
        
        # File handler
        log_file = log_dir / f"agent_mode_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)
        
        # Formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
    
    def _verify_main_app_components(self):
        """Verify that main app has all required components for Agent Mode."""
        required_attrs = ['data_processor', 'ml_engine', 'neural_visualizer']
        missing_attrs = []
        
        for attr in required_attrs:
            if not hasattr(self.main_app, attr) or getattr(self.main_app, attr) is None:
                missing_attrs.append(attr)
        
        if missing_attrs:
            self._add_log(f"âš ï¸ Warning: Missing main app components: {', '.join(missing_attrs)}")
        else:
            self._add_log("âœ… All main app components verified")
        
    def start_agent_mode(self, callback: Optional[Callable] = None):
        """
        Start the autonomous agent mode pipeline with user instruction support.
        
        Args:
            callback: Optional callback function to update UI
        """
        if self.is_running:
            return False, "Agent Mode is already running"
        
        # STEP 1: Get user instructions BEFORE starting pipeline
        if self.instruction_handler and hasattr(self.main_app, 'show_instruction_dialog'):
            self._add_log("ðŸ“ Collecting user instructions...")
            try:
                self.user_instructions = self.instruction_handler.get_user_instructions(
                    self.main_app.show_instruction_dialog
                )
                
                if self.user_instructions.get('raw_input'):
                    self._add_log(f"âœ… User instruction: \"{self.user_instructions['raw_input']}\"")
                    self._add_log("ðŸ§  Analyzing instruction and creating execution plan...")
                else:
                    self._add_log("â„¹ï¸ No specific instructions provided. Using intelligent defaults.")
            except Exception as e:
                self._add_log(f"âš ï¸ Could not get user instructions: {str(e)}")
                self._add_log("â„¹ï¸ Proceeding with intelligent defaults...")
        
        self.is_running = True
        self.progress = 0
        self.retry_count = 0
        self.errors = []
        self.log = []
        self.success_count = 0
        
        # Start pipeline in separate thread
        thread = threading.Thread(
            target=self._run_pipeline_with_recovery,
            args=(callback,),
            daemon=True
        )
        thread.start()
        
        return True, "Agent Mode started successfully"
        
    def stop_agent_mode(self):
        """Stop the agent mode pipeline."""
        self.is_running = False
        self.logger.info("Agent Mode stopped by user")
        
    def get_status(self) -> Dict[str, Any]:
        """Get current status of agent mode."""
        return {
            'is_running': self.is_running,
            'current_step': self.current_step,
            'progress': self.progress,
            'success_count': self.success_count,
            'total_steps': self.total_steps,
            'errors': self.errors[-5:],  # Last 5 errors
            'log': self.log[-10:],  # Last 10 log entries
            'retry_count': self.retry_count
        }
        
    def _run_pipeline_with_recovery(self, callback: Optional[Callable] = None):
        """
        Run the INTELLIGENT pipeline with adaptive decision-making and error recovery.
        
        Args:
            callback: Optional callback function to update UI
        """
        try:
            self.logger.info("Starting TRUE AUTONOMOUS Agent Mode Pipeline")
            self._add_log("ðŸ¤– TRUE AUTONOMOUS AGENT MODE - Analyzing data and planning strategy...")
            
            # PHASE 1: Intelligent Analysis and Planning
            if not self._intelligent_planning_phase():
                self._add_log("âŒ Planning phase failed - cannot proceed")
                return
            
            # PHASE 2: Execute dynamically generated pipeline
            for step_index, (step_name, reasoning) in enumerate(self.pipeline_steps):
                if not self.is_running:
                    break
                
                self.current_step = step_name
                self.progress = int((step_index / len(self.pipeline_steps)) * 100)
                
                # Log AI reasoning for this step
                self._add_log(f"ðŸ§  DECISION: {step_name}")
                self._add_log(f"   REASONING: {reasoning}")
                
                if callback:
                    callback(self.get_status())
                
                # Execute step with intelligent retry and adaptation
                self._add_log(f"\n{'='*70}")
                self._add_log(f"ðŸ”„ EXECUTING STEP {step_index+1}/{len(self.pipeline_steps)}: {step_name}")
                self._add_log(f"{'='*70}")
                
                success = self._execute_intelligent_step(step_name, reasoning)
                
                if success:
                    self.success_count += 1
                    self._add_log(f"âœ… {step_name} completed successfully")
                    self.logger.info(f"Step completed: {step_name}")
                    
                    # Record successful decision
                    try:
                        self.decision_engine.record_decision(
                            step_name, reasoning, 'success', 
                            f"Step executed successfully with chosen strategy"
                        )
                    except Exception as e:
                        # Don't fail the whole pipeline if recording fails
                        self.logger.warning(f"Could not record decision: {str(e)}")
                else:
                    self._add_log(f"âš ï¸ {step_name} FAILED - Agent adapting strategy...")
                    self.logger.warning(f"Step failed: {step_name}")
                    
                    # Try alternative strategy
                    alt_success = self._try_alternative_strategy(step_name)
                    if alt_success:
                        self.success_count += 1
                        self._add_log(f"âœ… Alternative strategy succeeded for {step_name}")
                    else:
                        self._add_log(f"âŒ {step_name} PERMANENTLY FAILED - continuing with next step")
                        self._add_log(f"âŒ Training will not be available in Q&A until this is fixed!")
                        try:
                            self.decision_engine.record_decision(
                                step_name, reasoning, 'failure',
                                f"All strategies failed for this step"
                            )
                        except Exception as e:
                            self.logger.warning(f"Could not record decision: {str(e)}")
            
            # PHASE 3: Final optimization and learning
            self._learning_phase()
            
            # Final status update
            self.progress = 100
            self.current_step = "Completed"
            self._add_log(f"ðŸŽ‰ Pipeline completed! {self.success_count}/{len(self.pipeline_steps)} steps successful")
            self._add_log(f"ðŸ“Š Quality Score: {self.dataset_profile.get('quality_score', 0):.1f}/100")
            self.logger.info("Agent Mode Pipeline completed with learning")
            
            # Call status update BEFORE setting is_running to False
            # This ensures UI knows agent was running when it completed
            if callback:
                callback(self.get_status())
            
            # Now mark as not running
            self.is_running = False
            
            # Save learned knowledge
            try:
                self.decision_engine.save_knowledge_base()
                self._add_log("ðŸ’¾ Knowledge base saved for future learning")
            except Exception as e:
                self._add_log(f"âš ï¸ Could not save knowledge base: {str(e)}")
                
        except Exception as e:
            self.logger.error(f"Critical pipeline error: {str(e)}")
            self._add_error(f"Critical pipeline error: {str(e)}")
    
    def _intelligent_planning_phase(self) -> bool:
        """
        PHASE 1: Analyze data and intelligently plan the pipeline.
        Incorporates user instructions with ReAct-style reasoning.
        """
        try:
            self._add_log("ðŸ” PHASE 1: Intelligent Analysis & Planning with User Instructions")
            
            # Load data if not already loaded
            if self.main_app.current_data is None:
                self._add_log("ðŸ“‚ No data loaded - generating sample data")
                sample_data = self._generate_sample_data()
                if sample_data is not None:
                    self.main_app.current_data = sample_data
                else:
                    return False
            
            # STEP 1: Apply user filters if specified
            # Apply filters if user specified them
            if self.instruction_handler and self.user_instructions:
                raw_instruction = self.user_instructions.get('raw_input', '')
                if raw_instruction:
                    parsed = self.instruction_handler._parse_instructions(raw_instruction)
                    # Check if any filter instructions exist
                    filter_items = [item for item in parsed if item['type'].name == 'FILTER_DATA']
                    
                    if filter_items:
                        self._add_log("ðŸ” Applying user-specified filters...")
                        try:
                            filter_criteria = filter_items[0]['details']
                            self.main_app.current_data = self.instruction_handler.apply_filters_to_data(
                                self.main_app.current_data, filter_criteria
                            )
                            self._add_log(f"   âœ… Filtered data: {self.main_app.current_data.shape}")
                            self.instruction_handler.log_context('filter_data', 
                                f"Applied filters: {filter_criteria}, Result shape: {self.main_app.current_data.shape}")
                        except Exception as e:
                            self._add_log(f"   âš ï¸ Filter error: {str(e)}, using full dataset")
            
            # Create comprehensive dataset profile
            self._add_log("ðŸ“Š Analyzing dataset characteristics...")
            self.dataset_profile = self.decision_engine.analyze_dataset_profile(
                self.main_app.current_data
            )
            
            # Log key insights
            self._add_log(f"   ðŸ“ Size: {self.dataset_profile['shape']} ({self.dataset_profile['size_category']})")
            self._add_log(f"   ðŸ“Š Quality Score: {self.dataset_profile['quality_score']:.1f}/100")
            self._add_log(f"   ðŸ§© Complexity: {self.dataset_profile['complexity']}")
            self._add_log(f"   ðŸ”¢ Numeric: {self.dataset_profile['n_numeric']}, Categorical: {self.dataset_profile['n_categorical']}")
            self._add_log(f"   ðŸ•³ï¸ Missing: {self.dataset_profile['missing_ratio']:.1%}")
            
            # STEP 2: Create execution plan based on user instructions + dataset analysis
            # Only use custom planning if user provided actual instructions (not just clicked "Skip")
            has_user_instructions = (self.instruction_handler and 
                                    self.user_instructions and 
                                    self.user_instructions.get('raw_input', '').strip())
            
            if has_user_instructions:
                self._add_log("ðŸ§  Creating ReAct-style execution plan...")
                
                # Extract raw instruction and parse it
                raw_instruction = self.user_instructions.get('raw_input', '')
                parsed_instructions = []
                
                if raw_instruction:
                    # Parse the instruction to get structured components
                    parsed_instructions = self.instruction_handler._parse_instructions(raw_instruction)
                    self._add_log(f"ðŸ“ Parsed {len(parsed_instructions)} instruction components")
                
                # Create execution plan with parsed instructions
                self.execution_plan = self.instruction_handler.create_execution_plan(
                    raw_instruction, parsed_instructions, self.dataset_profile
                )
                
                # Log the reasoning (new structure)
                if 'reasoning' in self.execution_plan:
                    intent = self.execution_plan['reasoning'].get('intent', '')
                    if intent:
                        self._add_log(f"ðŸ’­ Intent: {intent[:150]}...")
                    
                    strategy = self.execution_plan['reasoning'].get('strategy', '')
                    if strategy:
                        self._add_log(f"ðŸŽ¯ Strategy: {strategy}")
                
                # Extract action steps from new structure
                action_steps = self.execution_plan.get('steps', [])
                
                # Convert to pipeline steps
                self.pipeline_steps = []
                for action in action_steps:
                    step_name = action['action'].replace('_', ' ').title()
                    thought = action.get('thought', '')
                    self.pipeline_steps.append((step_name, thought))
                
                self._add_log(f"âœ… Custom plan created: {len(self.pipeline_steps)} steps based on user instructions")
            else:
                # Default intelligent planning
                self._add_log("ðŸ§  Agent deciding optimal pipeline...")
                pipeline_decisions = self.decision_engine.decide_pipeline_steps(self.dataset_profile)
                
                # Convert decisions to executable steps
                self.pipeline_steps = []
                for step_name, reasoning in pipeline_decisions:
                    # Map decision to actual function
                    step_function = self._map_step_to_function(step_name)
                    if step_function:
                        self.pipeline_steps.append((step_name, reasoning))
                
                self._add_log(f"âœ… Intelligent plan created: {len(self.pipeline_steps)} adaptive steps")
            
            self.total_steps = len(self.pipeline_steps)
            
            # Log the plan
            for i, (step_name, reasoning) in enumerate(self.pipeline_steps, 1):
                self._add_log(f"   {i}. {step_name}: {reasoning[:80]}...")
            
            # Select cleaning strategies (consider user preferences)
            self.selected_strategies = self.decision_engine.select_cleaning_strategy(
                self.dataset_profile
            )
            
            # Override with user preferences if specified
            if self.user_instructions.get('cleaning_level'):
                self._add_log(f"ðŸ§¹ Using user-specified cleaning level: {self.user_instructions['cleaning_level']}")
            
            return True
            
        except Exception as e:
            self._add_log(f"âŒ Planning phase error: {str(e)}")
            self.logger.error(f"Planning phase error: {str(e)}")
            traceback.print_exc()
            return False
    
    def _map_step_to_function(self, step_name: str):
        """Map step name to actual function - used for routing."""
        # We'll use step name matching in _execute_intelligent_step
        return True
    
    def _execute_intelligent_step(self, step_name: str, reasoning: str) -> bool:
        """
        Execute a step with intelligent decision-making.
        """
        try:
            # Route to appropriate handler based on step name (case-insensitive)
            step_lower = step_name.lower()
            
            if "load" in step_lower or ("data" in step_lower and "load" in reasoning.lower()):
                # Data is already loaded, just return success
                self._add_log("   â„¹ï¸ Data already loaded, skipping")
                return True
            elif "filter" in step_lower:
                # Filters already applied in planning phase
                self._add_log("   â„¹ï¸ Filters already applied, skipping")
                return True
            elif "column" in step_lower and "select" in step_lower:
                # Column selection
                self._add_log("   â„¹ï¸ Column selection handled, skipping")
                return True
            elif "analysis" in step_lower or "analyze" in step_lower:
                return self._analyze_data_step()
            elif "clean" in step_lower:
                return self._intelligent_clean_data_step(step_name)
            elif "feature" in step_lower and "engineering" in step_lower:
                return self._intelligent_feature_engineering_step(step_name)
            elif "train" in step_lower or "model" in step_lower:
                return self._intelligent_train_models_step()
            elif "evaluat" in step_lower:
                return self._evaluate_models_step()
            elif "visualiz" in step_lower or "export" in step_lower:
                # Combined visualization and export step
                if "visualiz" in step_lower:
                    self._intelligent_generate_visualizations_step(step_name)
                if "export" in step_lower:
                    self._export_results_step()
                return True
            else:
                # Generic execution for unknown steps
                self._add_log(f"   â„¹ï¸ Generic step: {step_name}")
                return True
                
        except Exception as e:
            self._add_log(f"âŒ Step execution error: {str(e)}")
            return False
    
    def _try_alternative_strategy(self, step_name: str) -> bool:
        """
        Try an alternative strategy when primary strategy fails.
        This demonstrates adaptive decision-making.
        """
        try:
            self._add_log(f"ðŸ”„ Trying alternative strategy for: {step_name}")
            
            if "Cleaning" in step_name:
                # Try simpler cleaning approach
                self._add_log("   â†’ Switching to basic cleaning strategy")
                return self._basic_clean_data_step()
            elif "Training" in step_name:
                # Try simpler model
                self._add_log("   â†’ Switching to simpler models")
                return self._basic_train_models_step()
            else:
                return False
                
        except Exception as e:
            self._add_log(f"   âŒ Alternative strategy also failed: {str(e)}")
            return False
    
    def _learning_phase(self):
        """
        PHASE 3: Learn from this run and update knowledge base.
        """
        try:
            self._add_log("ðŸ“š PHASE 3: Learning & Knowledge Update")
            
            # Record dataset profile for future reference
            self.decision_engine.knowledge_base['dataset_profiles'].append({
                'timestamp': datetime.now().isoformat(),
                'profile': self.dataset_profile,
                'success_rate': self.success_count / len(self.pipeline_steps) if self.pipeline_steps else 0,
                'strategies_used': self.selected_strategies
            })
            
            # Analyze what worked well
            successful_steps = [step for step, _ in self.pipeline_steps[:self.success_count]]
            self._add_log(f"   âœ… Successful strategies: {len(successful_steps)}/{len(self.pipeline_steps)}")
            
            # Save insights
            self.decision_engine.save_knowledge_base()
            self._add_log("   ðŸ’¾ Knowledge base updated with new insights")
            
        except Exception as e:
            self._add_log(f"âš ï¸ Learning phase error: {str(e)}")
            
    def _execute_step_with_retry(self, step_name: str, step_function: Callable) -> bool:
        """
        Execute a pipeline step with retry mechanism.
        
        Args:
            step_name: Name of the step
            step_function: Function to execute
            
        Returns:
            bool: True if successful, False if failed after all retries
        """
        for attempt in range(self.max_retries + 1):
            try:
                self.logger.info(f"Executing {step_name} (attempt {attempt + 1})")
                
                # Execute the step
                result = step_function()
                
                if result:
                    return True
                else:
                    raise Exception(f"{step_name} returned False")
                    
            except Exception as e:
                error_msg = f"{step_name} attempt {attempt + 1} failed: {str(e)}"
                self.logger.warning(error_msg)
                self._add_error(error_msg)
                
                if attempt < self.max_retries:
                    # Wait before retry (exponential backoff)
                    wait_time = 2 ** attempt
                    time.sleep(wait_time)
                    self._add_log(f"Retrying {step_name} in {wait_time} seconds...")
                else:
                    return False
                    
        return False
        
    def _add_log(self, message: str):
        """Add a log entry with timestamp."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        log_entry = f"[{timestamp}] {message}"
        self.log.append(log_entry)
        
    def _add_error(self, error: str):
        """Add an error entry with timestamp."""
        timestamp = datetime.now().strftime("%H:%M:%S")
        error_entry = f"[{timestamp}] ERROR: {error}"
        self.errors.append(error_entry)
        
    # Pipeline Step Implementations
    
    def _load_data_step(self) -> bool:
        """Load data step with automatic format detection."""
        try:
            if self.main_app.current_data is not None:
                return True
                
            # Try to load sample data if no data is loaded
            sample_data = self._generate_sample_data()
            if sample_data is not None:
                self.main_app.current_data = sample_data
                self._add_log("Sample data generated for demonstration")
                return True
                
            return False
            
        except Exception as e:
            raise Exception(f"Data loading failed: {str(e)}")
            
    def _analyze_data_step(self) -> bool:
        """Analyze data structure and characteristics."""
        try:
            if self.main_app.current_data is None:
                return False
                
            # Perform data analysis using existing methods
            if hasattr(self.main_app, 'analyze_data'):
                self.main_app.analyze_data()
                
            return True
            
        except Exception as e:
            raise Exception(f"Data analysis failed: {str(e)}")
    
    def _intelligent_clean_data_step(self, step_name: str) -> bool:
        """
        INTELLIGENT data cleaning based on selected strategy.
        """
        try:
            if self.main_app.current_data is None:
                return False
            
            strategy = self.selected_strategies.get('missing', 'smart_imputation')
            self._add_log(f"ðŸ§¹ Applying {strategy} strategy for {step_name}")
            
            # Use existing cleaning but log the decision
            return self._clean_data_step()
            
        except Exception as e:
            raise Exception(f"Intelligent cleaning failed: {str(e)}")
    
    def _basic_clean_data_step(self) -> bool:
        """
        Fallback basic cleaning when advanced cleaning fails.
        """
        try:
            if self.main_app.current_data is None:
                return False
                
            self._add_log("ðŸ§¹ Applying basic cleaning (dropna + fillna)")
            
            # Simple cleaning
            df = self.main_app.current_data.copy()
            
            # Drop rows with too many missing values
            threshold = 0.5 * df.shape[1]
            df = df.dropna(thresh=threshold)
            
            # Fill remaining missing values
            for col in df.columns:
                if df[col].dtype in ['float64', 'int64']:
                    df[col].fillna(df[col].median(), inplace=True)
                else:
                    df[col].fillna(df[col].mode()[0] if len(df[col].mode()) > 0 else 'Unknown', inplace=True)
            
            self.main_app.cleaned_data = df
            self.main_app.processed_data = df
            self._add_log(f"âœ… Basic cleaning complete: {df.shape}")
            
            return True
            
        except Exception as e:
            self._add_log(f"âŒ Basic cleaning failed: {str(e)}")
            return False
    
    def _intelligent_feature_engineering_step(self, step_name: str) -> bool:
        """
        INTELLIGENT feature engineering based on data type.
        """
        try:
            self._add_log(f"ðŸ”§ Intelligent feature engineering: {step_name}")
            
            if "Text" in step_name:
                self._add_log("   â†’ Text feature extraction activated")
            elif "Time Series" in step_name:
                self._add_log("   â†’ Temporal feature generation activated")
            elif "Categorical" in step_name:
                self._add_log("   â†’ Advanced categorical encoding activated")
            
            # Call existing feature engineering if available
            return self._feature_engineering_step()
            
        except Exception as e:
            self._add_log(f"âš ï¸ Feature engineering skipped: {str(e)}")
            return True  # Non-critical, continue
    
    def _intelligent_train_models_step(self) -> bool:
        """
        INTELLIGENT model training with algorithm selection.
        """
        try:
            self._add_log("=" * 70)
            self._add_log("ðŸš€ STARTING MODEL TRAINING STEP")
            self._add_log("=" * 70)
            
            # Check for cleaned data
            if self.main_app.cleaned_data is None:
                self._add_log("âŒ TRAINING FAILED: No cleaned_data available!")
                self._add_log(f"   current_data exists: {self.main_app.current_data is not None}")
                if self.main_app.current_data is not None:
                    self._add_log(f"   Using current_data as fallback: {self.main_app.current_data.shape}")
                    self.main_app.cleaned_data = self.main_app.current_data.copy()
                    self.main_app.processed_data = self.main_app.current_data.copy()
                else:
                    self._add_log("   No data available at all - cannot train!")
                    return False
            
            self._add_log(f"âœ… Cleaned data available: {self.main_app.cleaned_data.shape}")
            self._add_log(f"   Columns: {list(self.main_app.cleaned_data.columns)[:10]}...")
            
            # Auto-select target column from CLEANED data
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                self._add_log("ðŸŽ¯ Auto-selecting target column from cleaned data...")
                self._auto_select_target_column()
                self._add_log(f"   Selected: {self.main_app.target_column}")
            else:
                # CRITICAL: If target exists but not present in cleaned_data, re-select
                if self.main_app.target_column not in self.main_app.cleaned_data.columns:
                    self._add_log(f"âš ï¸ Target '{self.main_app.target_column}' NOT in cleaned_data - re-selecting")
                    self._auto_select_target_column()
                    self._add_log(f"   Re-selected: {self.main_app.target_column}")
            
            # Auto-detect problem type
            if hasattr(self.main_app, 'problem_type_var'):
                if self.main_app.problem_type_var.get() == "auto":
                    self._add_log("ðŸ§  Auto-detecting problem type...")
                    self._auto_detect_problem_type()
            
            problem_type = getattr(self.main_app, 'problem_type_var', None)
            problem_type = problem_type.get() if problem_type else 'classification'
            self._add_log(f"ðŸ“Š Problem type: {problem_type}")
            
            # Select optimal algorithms
            algorithms = self.decision_engine.select_ml_algorithms(
                self.dataset_profile, 
                problem_type
            )
            self._add_log(f"ðŸ¤– Selected algorithms: {', '.join(algorithms)}")
            
            # Call existing training
            self._add_log("ðŸ”„ Calling _train_models_step()...")
            result = self._train_models_step()
            
            if result:
                self._add_log("=" * 70)
                self._add_log("âœ… MODEL TRAINING COMPLETED SUCCESSFULLY!")
                self._add_log("=" * 70)
            else:
                self._add_log("=" * 70)
                self._add_log("âŒ MODEL TRAINING FAILED!")
                self._add_log("=" * 70)
            
            return result
            
        except Exception as e:
            self._add_log("=" * 70)
            self._add_log(f"âŒ EXCEPTION: {str(e)}")
            import traceback
            self._add_log(f"Traceback: {traceback.format_exc()}")
            self._add_log("=" * 70)
            raise Exception(f"Intelligent training failed: {str(e)}")
    
    def _basic_train_models_step(self) -> bool:
        """
        Fallback basic training with simple model.
        """
        try:
            self._add_log("ðŸ¤– Fallback: Training simple logistic regression")
            
            # Check for cleaned data
            if self.main_app.cleaned_data is None:
                self._add_log("âŒ No cleaned data for basic training either!")
                return False
            
            self._add_log(f"âœ… Using cleaned data: {self.main_app.cleaned_data.shape}")
            self._add_log(f"   Columns: {list(self.main_app.cleaned_data.columns)}")
            
            # Auto-select target if needed
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                self._auto_select_target_column()
            
            self._add_log(f"ðŸŽ¯ Target column requested: {self.main_app.target_column}")
            
            # CRITICAL: Check if target exists in cleaned data
            if self.main_app.target_column not in self.main_app.cleaned_data.columns:
                self._add_log(f"âŒ ERROR: Target '{self.main_app.target_column}' not in cleaned data!")
                self._add_log(f"   Available columns: {list(self.main_app.cleaned_data.columns)}")
                
                # Re-select from ACTUAL columns
                self._add_log("   ðŸ”„ Re-selecting target from cleaned data columns...")
                for col in ['Depression', 'Suicidal Thoughts', 'Have you ever had suicidal thoughts ?', 'CGPA']:
                    if col in self.main_app.cleaned_data.columns:
                        self.main_app.target_column = col
                        self._add_log(f"   âœ… New target: {col}")
                        break
                else:
                    # Use first column as fallback
                    self.main_app.target_column = self.main_app.cleaned_data.columns[0]
                    self._add_log(f"   âš ï¸ Using first column as target: {self.main_app.target_column}")
            
            self._add_log(f"ðŸŽ¯ Final target: {self.main_app.target_column}")
            
            # Prepare data
            X = self.main_app.cleaned_data.drop(columns=[self.main_app.target_column])
            y = self.main_app.cleaned_data[self.main_app.target_column]
            
            # Train simple model
            from sklearn.linear_model import LogisticRegression
            from sklearn.model_selection import train_test_split
            from sklearn.metrics import accuracy_score
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            # Handle categorical columns
            X_train_numeric = X_train.select_dtypes(include=[np.number])
            X_test_numeric = X_test.select_dtypes(include=[np.number])
            
            model = LogisticRegression(max_iter=1000)
            model.fit(X_train_numeric.fillna(0), y_train)
            
            y_pred = model.predict(X_test_numeric.fillna(0))
            accuracy = accuracy_score(y_test, y_pred)
            
            self._add_log(f"âœ… Basic model trained! Accuracy: {accuracy:.4f}")
            self.main_app.training_complete = True
            
            return True
            
        except Exception as e:
            self._add_log(f"âŒ Basic training failed: {str(e)}")
            import traceback
            self._add_log(f"Traceback: {traceback.format_exc()}")
            return False
    
    def _intelligent_generate_visualizations_step(self, step_name: str) -> bool:
        """
        INTELLIGENT visualization generation based on dataset size.
        """
        try:
            if "Comprehensive" in step_name:
                self._add_log("ðŸŽ¨ Generating comprehensive visualizations")
            else:
                self._add_log("ðŸŽ¨ Generating essential visualizations (dataset is large)")
            
            return self._generate_visualizations_step()
            
        except Exception as e:
            raise Exception(f"Visualization generation failed: {str(e)}")
            
    def _clean_data_step(self) -> bool:
        """Clean and preprocess data."""
        try:
            if self.main_app.current_data is None:
                return False
                
            self._add_log("Starting intelligent data cleaning...")
            
            # Ensure data processor has the current data
            if hasattr(self.main_app, 'data_processor') and self.main_app.data_processor:
                # Make sure the data processor has the current data loaded
                if not hasattr(self.main_app.data_processor, 'original_data') or self.main_app.data_processor.original_data is None:
                    # Use load_and_analyze_dataframe instead of direct assignment
                    self.main_app.data_processor.load_and_analyze_dataframe(
                        self.main_app.current_data, 
                        "Agent Mode Data"
                    )
                    self._add_log("Loaded and analyzed data in processor")
                
                # Direct call to data processor's smart cleaning
                self._add_log("Executing smart data cleaning algorithms...")
                success = self.main_app.data_processor.smart_clean_data()
                
                if success and hasattr(self.main_app.data_processor, 'cleaned_data') and self.main_app.data_processor.cleaned_data is not None:
                    self.main_app.cleaned_data = self.main_app.data_processor.cleaned_data.copy()
                    self._add_log(f"âœ… Data cleaned: {self.main_app.cleaned_data.shape[0]} rows Ã— {self.main_app.cleaned_data.shape[1]} columns")
                    self._add_log(f"   Columns in cleaned data: {list(self.main_app.cleaned_data.columns)[:10]}...")
                    
                    # CRITICAL: Update the main app's processed data for ML pipeline
                    self.main_app.processed_data = self.main_app.cleaned_data.copy()
                    
                    # Calculate cleaning stats
                    original_rows = self.main_app.current_data.shape[0]
                    cleaned_rows = self.main_app.cleaned_data.shape[0]
                    if cleaned_rows < original_rows:
                        removed = original_rows - cleaned_rows
                        self._add_log(f"ðŸ—‘ï¸ Removed {removed} problematic rows ({removed/original_rows*100:.1f}%)")
                    
                    self._add_log("Data cleaning completed successfully!")
                    return True
                else:
                    raise Exception("Data processor smart_clean_data failed or returned no cleaned data")
            else:
                raise Exception("Data processor not available - cannot perform intelligent cleaning")
                
        except Exception as e:
            self._add_log(f"âŒ Data cleaning error: {str(e)}")
            raise Exception(f"Data cleaning failed: {str(e)}")
            
    def _feature_engineering_step(self) -> bool:
        """Perform feature engineering."""
        try:
            if hasattr(self.main_app, 'advanced_feature_engineering'):
                self.main_app.advanced_feature_engineering()
                
            return True
            
        except Exception as e:
            # Feature engineering is optional, so continue if it fails
            self._add_log(f"Feature engineering skipped: {str(e)}")
            return True
            
    def _train_models_step(self) -> bool:
        """Train machine learning models."""
        try:
            self._add_log("ðŸ” Checking training prerequisites...")
            
            # Ensure we have cleaned data
            if self.main_app.cleaned_data is None:
                self._add_log("âŒ TRAINING BLOCKED: No cleaned data available")
                self._add_log(f"   current_data exists: {self.main_app.current_data is not None}")
                self._add_log(f"   cleaned_data: {self.main_app.cleaned_data}")
                return False
            
            self._add_log(f"âœ… Cleaned data available: {self.main_app.cleaned_data.shape}")
            self._add_log(f"   Columns: {list(self.main_app.cleaned_data.columns)[:5]}...")
            
            # Auto-select target column if not selected
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                self._add_log("ðŸŽ¯ Auto-selecting target column...")
                self._auto_select_target_column()
            
            # Auto-set problem type if not set
            if hasattr(self.main_app, 'problem_type_var'):
                if self.main_app.problem_type_var.get() == "auto":
                    self._add_log("ðŸ§  Auto-detecting problem type...")
                    self._auto_detect_problem_type()
            
            # Verify target column is in cleaned data
            if self.main_app.target_column and self.main_app.target_column not in self.main_app.cleaned_data.columns:
                self._add_log(f"âš ï¸ Target '{self.main_app.target_column}' NOT in cleaned data!")
                self._add_log(f"   Cleaned data columns: {list(self.main_app.cleaned_data.columns)}")
                self._add_log("   ðŸ”„ Re-selecting target from cleaned data...")
                
                # Try common target names that exist in cleaned data
                for col in ['Depression', 'Suicidal Thoughts', 'Have you ever had suicidal thoughts ?', 'CGPA']:
                    if col in self.main_app.cleaned_data.columns:
                        self.main_app.target_column = col
                        self._add_log(f"   âœ… Found target: {col}")
                        break
                else:
                    # Fallback: use first column
                    self.main_app.target_column = self.main_app.cleaned_data.columns[0]
                    self._add_log(f"   âš ï¸ Using first available column: {self.main_app.target_column}")
            
            self._add_log(f"ðŸŽ¯ Selected Target: {self.main_app.target_column}")
            self._add_log(f"ðŸ“Š Problem Type: {self.main_app.problem_type_var.get() if hasattr(self.main_app, 'problem_type_var') else 'auto'}")
            
            # Get problem type
            problem_type_str = self.main_app.problem_type_var.get() if hasattr(self.main_app, 'problem_type_var') else 'classification'
            
            # Check for ML engine
            has_ml_engine = hasattr(self.main_app, 'ml_engine') and self.main_app.ml_engine
            self._add_log(f"ðŸ”§ ML Engine available: {has_ml_engine}")
            
            # Train models using ML engine
            if has_ml_engine:
                self._add_log("ðŸš€ Starting model training with ML engine...")

                # Use ML engine prepare_data + train_all_models to match SmartMLEngine API
                try:
                    self._add_log("   Preparing data in ml_engine.prepare_data()...")
                    # Pass problem_type to prepare_data
                    prep_success = self.main_app.ml_engine.prepare_data(
                        self.main_app.cleaned_data, 
                        self.main_app.target_column, 
                        problem_type_str
                    )
                    self._add_log(f"   prepare_data() returned: {prep_success}")
                    if not prep_success:
                        self._add_log("   âŒ ML engine data preparation failed")
                        return False

                    self._add_log("   Starting ml_engine.train_all_models()...")
                    train_success = self.main_app.ml_engine.train_all_models(
                        quick_mode=False, 
                        automl=False, 
                        deep_learning=False, 
                        cross_validation=True, 
                        feature_selection=True
                    )
                    self._add_log(f"   train_all_models() returned: {train_success}")

                    if train_success:
                        self.main_app.training_complete = True
                        self._add_log("âœ… Model training completed successfully!")

                        # Log results
                        if hasattr(self.main_app.ml_engine, 'results') and self.main_app.ml_engine.results:
                            self._add_log(f"ðŸ“Š Trained {len(self.main_app.ml_engine.results)} models")
                            for model_name, metrics in self.main_app.ml_engine.results.items():
                                acc = metrics.get('accuracy', metrics.get('r2_score', 0))
                                try:
                                    self._add_log(f"   â€¢ {model_name}: {acc:.4f}")
                                except Exception:
                                    self._add_log(f"   â€¢ {model_name}: {acc}")
                        return True
                    else:
                        self._add_log("âš ï¸ ML engine reported training failure")
                        return False
                except Exception as e:
                    self._add_log(f"âŒ Exception while calling ML engine: {str(e)}")
                    import traceback
                    self._add_log(f"Traceback: {traceback.format_exc()}")
                    return False
            elif hasattr(self.main_app, 'train_models'):
                # Fallback to main app's train_models method
                self._add_log("ðŸš€ Using main app training method...")
                self.main_app.train_models()
                self.main_app.training_complete = True
                return True
            else:
                self._add_log("âŒ No ML engine or training method available")
                return False
                
        except Exception as e:
            self._add_log(f"âŒ Training error: {str(e)}")
            import traceback
            self._add_log(f"Traceback: {traceback.format_exc()}")
            return False
            
    def _auto_select_target_column(self):
        """Automatically select a suitable target column."""
        try:
            # CRITICAL: Use cleaned_data if available, otherwise current_data
            if hasattr(self.main_app, 'cleaned_data') and self.main_app.cleaned_data is not None:
                data = self.main_app.cleaned_data
                self._add_log("   ðŸ“‹ Using cleaned_data for target selection")
            elif self.main_app.current_data is not None:
                data = self.main_app.current_data
                self._add_log("   ðŸ“‹ Using current_data for target selection")
            else:
                self._add_log("   âŒ No data available for target selection!")
                return
            
            self._add_log(f"   ðŸ“Š Available columns: {list(data.columns)[:10]}...")
            
            # Look for common target column names
            target_candidates = []
            common_target_names = ['target', 'label', 'class', 'outcome', 'result', 'y', 
                                 'prediction', 'score', 'rating', 'price', 'value',
                                 'suicidal', 'depression', 'anxiety', 'stress']
            
            for col in data.columns:
                col_lower = col.lower()
                for target_name in common_target_names:
                    if target_name in col_lower:
                        target_candidates.append((col, target_name))
                        break
            
            # If we found candidates, pick the best one
            if target_candidates:
                # Sort by priority (exact matches first)
                target_candidates.sort(key=lambda x: len(x[1]), reverse=True)
                selected_target = target_candidates[0][0]
                self._add_log(f"   âœ… Found target candidate: {selected_target}")
            else:
                # Fall back to last column if it looks like a target
                last_col = data.columns[-1]
                # Check if last column has few unique values (likely categorical target)
                if data[last_col].nunique() < len(data) * 0.1:  # Less than 10% unique values
                    selected_target = last_col
                    self._add_log(f"   âœ… Using last column as target: {selected_target}")
                else:
                    # Use first numerical column with reasonable range
                    numeric_cols = data.select_dtypes(include=[np.number]).columns
                    if len(numeric_cols) > 0:
                        selected_target = numeric_cols[0]
                        self._add_log(f"   âœ… Using first numeric column: {selected_target}")
                    else:
                        selected_target = data.columns[-1]  # Last resort
                        self._add_log(f"   âš ï¸ Last resort - using: {selected_target}")
            
            # Set the target column
            self.main_app.target_column = selected_target
            if hasattr(self.main_app, 'target_var'):
                self.main_app.target_var.set(selected_target)
                
            self._add_log(f"Auto-selected target column: {selected_target}")
            
        except Exception as e:
            self._add_log(f"Failed to auto-select target: {str(e)}")
            
    def _auto_detect_problem_type(self):
        """Automatically detect problem type based on target column."""
        try:
            if not hasattr(self.main_app, 'target_column') or not self.main_app.target_column:
                return
                
            target_col = self.main_app.target_column
            # Prefer cleaned_data when checking target distribution
            data_source = None
            if hasattr(self.main_app, 'cleaned_data') and self.main_app.cleaned_data is not None:
                data_source = self.main_app.cleaned_data
            elif self.main_app.current_data is not None:
                data_source = self.main_app.current_data
            else:
                return

            if target_col not in data_source.columns:
                return

            target_data = data_source[target_col]
            unique_values = target_data.nunique()
            
            # Classification if few unique values
            if unique_values <= 10 and target_data.dtype in ['object', 'category', 'bool']:
                problem_type = "classification"
            elif unique_values <= 20 and target_data.dtype in ['int64', 'int32']:
                problem_type = "classification"
            else:
                problem_type = "regression"
                
            if hasattr(self.main_app, 'problem_type_var'):
                self.main_app.problem_type_var.set(problem_type)
                
            self._add_log(f"Auto-detected problem type: {problem_type}")
            
        except Exception as e:
            self._add_log(f"Failed to auto-detect problem type: {str(e)}")
            
    def _evaluate_models_step(self) -> bool:
        """Evaluate trained models."""
        try:
            if hasattr(self.main_app, 'show_model_performance'):
                self.main_app.show_model_performance()
                
            return True
            
        except Exception as e:
            # Model evaluation is optional
            self._add_log(f"Model evaluation skipped: {str(e)}")
            return True
            
    def _generate_visualizations_step(self) -> bool:
        """Generate comprehensive visualizations."""
        try:
            self._add_log("ðŸŽ¨ Generating visualizations...")
            
            # Use thread-safe visualization updates
            if hasattr(self.main_app, 'root'):
                # Schedule visualization update on main thread
                self.main_app.root.after(0, self._safe_update_visualizations)
                
            # Wait a moment for visualizations to be generated
            import time
            time.sleep(2)
            
            self._add_log("âœ… Visualizations generated successfully")
            return True
            
        except Exception as e:
            raise Exception(f"Visualization generation failed: {str(e)}")
            
    def _safe_update_visualizations(self):
        """Thread-safe visualization update."""
        try:
            if hasattr(self.main_app, 'update_all_visualizations'):
                self.main_app.update_all_visualizations()
            elif hasattr(self.main_app, 'show_data_overview'):
                self.main_app.show_data_overview()
        except Exception as e:
            self._add_log(f"Visualization update failed: {str(e)}")
            
    def _export_results_step(self) -> bool:
        """Export results and artifacts."""
        try:
            self._add_log("ðŸ’¾ Exporting results and visualizations...")
            
            # Export visualizations if available
            if hasattr(self.main_app, 'advanced_viz_dashboard') and self.main_app.advanced_viz_dashboard:
                try:
                    self.main_app.advanced_viz_dashboard.export_all_viz()
                    self._add_log("âœ… All visualizations exported to folder")
                except Exception as viz_error:
                    self._add_log(f"âš ï¸ Visualization export partial: {str(viz_error)}")
            
            # Export cleaned data
            if hasattr(self.main_app, 'export_cleaned_data'):
                try:
                    self.main_app.export_cleaned_data()
                    self._add_log("âœ… Cleaned data exported")
                except Exception as data_error:
                    self._add_log(f"âš ï¸ Data export skipped: {str(data_error)}")
            
            # Export results summary
            if hasattr(self.main_app, 'export_results'):
                try:
                    self.main_app.export_results()
                    self._add_log("âœ… Results summary exported")
                except Exception as results_error:
                    self._add_log(f"âš ï¸ Results export skipped: {str(results_error)}")
                
            return True
            
        except Exception as e:
            # Export is optional, so we don't fail the pipeline
            self._add_log(f"âš ï¸ Export step completed with errors: {str(e)}")
            return True
            
    def _generate_sample_data(self) -> Optional[pd.DataFrame]:
        """Generate sample data for demonstration."""
        try:
            np.random.seed(42)
            n_samples = 1000
            
            data = {
                'feature1': np.random.normal(0, 1, n_samples),
                'feature2': np.random.exponential(2, n_samples),
                'feature3': np.random.uniform(-5, 5, n_samples),
                'category': np.random.choice(['A', 'B', 'C'], n_samples),
                'target': np.random.binomial(1, 0.3, n_samples)
            }
            
            # Add some correlations
            data['feature4'] = data['feature1'] * 0.5 + np.random.normal(0, 0.5, n_samples)
            data['target'] = ((data['feature1'] > 0) & (data['feature3'] > 0)).astype(int)
            
            return pd.DataFrame(data)
            
        except Exception as e:
            self.logger.error(f"Sample data generation failed: {str(e)}")
            return None